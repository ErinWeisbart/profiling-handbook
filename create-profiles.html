<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 5 Create Profiles | Image-based Profiling Handbook</title>
  <meta name="description" content="This is a handbook for processing image-based profiling datasets using CellProfiler and pycytominer" />
  <meta name="generator" content="bookdown 0.23.2 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 5 Create Profiles | Image-based Profiling Handbook" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is a handbook for processing image-based profiling datasets using CellProfiler and pycytominer" />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 5 Create Profiles | Image-based Profiling Handbook" />
  
  <meta name="twitter:description" content="This is a handbook for processing image-based profiling datasets using CellProfiler and pycytominer" />
  

<meta name="author" content="Beth Cimini, Tim Becker, Shantanu Singh, Gregory Way, Hamdah Abbasi" />


<meta name="date" content="2021-08-23" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="run-each-cellprofiler-step.html"/>
<link rel="next" href="appendix-a.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Image-based Profiling Handbook</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#collect-your-software"><i class="fa fa-check"></i><b>1.1</b> Collect your software</a></li>
<li class="chapter" data-level="1.2" data-path="introduction.html"><a href="introduction.html#collect-your-pipelines"><i class="fa fa-check"></i><b>1.2</b> Collect your pipelines</a></li>
<li class="chapter" data-level="1.3" data-path="introduction.html"><a href="introduction.html#determine-how-to-get-your-image-lists-to-cellprofiler"><i class="fa fa-check"></i><b>1.3</b> Determine how to get your image lists to CellProfiler</a></li>
<li class="chapter" data-level="1.4" data-path="introduction.html"><a href="introduction.html#execute-your-cellprofiler-pipelines"><i class="fa fa-check"></i><b>1.4</b> Execute your CellProfiler pipelines</a><ul>
<li class="chapter" data-level="1.4.1" data-path="introduction.html"><a href="introduction.html#optional-z-projection"><i class="fa fa-check"></i><b>1.4.1</b> (Optional) Z projection</a></li>
<li class="chapter" data-level="1.4.2" data-path="introduction.html"><a href="introduction.html#optional-qc"><i class="fa fa-check"></i><b>1.4.2</b> (Optional) QC</a></li>
<li class="chapter" data-level="1.4.3" data-path="introduction.html"><a href="introduction.html#illumination-correction"><i class="fa fa-check"></i><b>1.4.3</b> Illumination correction</a></li>
<li class="chapter" data-level="1.4.4" data-path="introduction.html"><a href="introduction.html#optional-assay-development"><i class="fa fa-check"></i><b>1.4.4</b> (Optional) Assay Development</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="introduction.html"><a href="introduction.html#analysis"><i class="fa fa-check"></i><b>1.5</b> Analysis</a></li>
<li class="chapter" data-level="1.6" data-path="introduction.html"><a href="introduction.html#aggregate-your-data"><i class="fa fa-check"></i><b>1.6</b> Aggregate your data</a></li>
<li class="chapter" data-level="1.7" data-path="introduction.html"><a href="introduction.html#create-and-manipulate-per-well-profiles."><i class="fa fa-check"></i><b>1.7</b> Create and manipulate per-well profiles.</a></li>
</ul></li>
<li class="part"><span><b>I Configuration</b></span></li>
<li class="chapter" data-level="2" data-path="configure-environment-for-full-profiling-pipeline.html"><a href="configure-environment-for-full-profiling-pipeline.html"><i class="fa fa-check"></i><b>2</b> Configure Environment for Full Profiling Pipeline</a><ul>
<li class="chapter" data-level="2.1" data-path="configure-environment-for-full-profiling-pipeline.html"><a href="configure-environment-for-full-profiling-pipeline.html#launch-an-aws-virtual-machine-for-making-csvs-and-running-distributed-cellprofiler"><i class="fa fa-check"></i><b>2.1</b> Launch an AWS Virtual Machine for making CSVs and running Distributed-CellProfiler</a></li>
<li class="chapter" data-level="2.2" data-path="configure-environment-for-full-profiling-pipeline.html"><a href="configure-environment-for-full-profiling-pipeline.html#create-a-tmux-session"><i class="fa fa-check"></i><b>2.2</b> Create a tmux session</a></li>
<li class="chapter" data-level="2.3" data-path="configure-environment-for-full-profiling-pipeline.html"><a href="configure-environment-for-full-profiling-pipeline.html#define-environment-variables"><i class="fa fa-check"></i><b>2.3</b> Define Environment Variables</a></li>
<li class="chapter" data-level="2.4" data-path="configure-environment-for-full-profiling-pipeline.html"><a href="configure-environment-for-full-profiling-pipeline.html#create-directories"><i class="fa fa-check"></i><b>2.4</b> Create Directories</a></li>
<li class="chapter" data-level="2.5" data-path="configure-environment-for-full-profiling-pipeline.html"><a href="configure-environment-for-full-profiling-pipeline.html#download-software"><i class="fa fa-check"></i><b>2.5</b> Download Software</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="setup-images.html"><a href="setup-images.html"><i class="fa fa-check"></i><b>3</b> Setup Images</a><ul>
<li class="chapter" data-level="3.1" data-path="setup-images.html"><a href="setup-images.html#upload-images"><i class="fa fa-check"></i><b>3.1</b> Upload Images</a></li>
<li class="chapter" data-level="3.2" data-path="setup-images.html"><a href="setup-images.html#prepare-images"><i class="fa fa-check"></i><b>3.2</b> Prepare Images</a></li>
<li class="chapter" data-level="3.3" data-path="setup-images.html"><a href="setup-images.html#create-list-of-plates"><i class="fa fa-check"></i><b>3.3</b> Create List of Plates</a></li>
<li class="chapter" data-level="3.4" data-path="setup-images.html"><a href="setup-images.html#create-loaddata-csvs"><i class="fa fa-check"></i><b>3.4</b> Create LoadData CSVs</a></li>
<li class="chapter" data-level="3.5" data-path="setup-images.html"><a href="setup-images.html#upload-image-location-files-to-s3"><i class="fa fa-check"></i><b>3.5</b> Upload image location files to S3</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="run-each-cellprofiler-step.html"><a href="run-each-cellprofiler-step.html"><i class="fa fa-check"></i><b>4</b> Run each CellProfiler step</a><ul>
<li class="chapter" data-level="4.1" data-path="run-each-cellprofiler-step.html"><a href="run-each-cellprofiler-step.html#upload-your-pipelines-to-s3"><i class="fa fa-check"></i><b>4.1</b> Upload your pipelines to S3</a></li>
<li class="chapter" data-level="4.2" data-path="run-each-cellprofiler-step.html"><a href="run-each-cellprofiler-step.html#configure-distributed-cellprofilers-run_batch_general-script"><i class="fa fa-check"></i><b>4.2</b> Configure Distributed-CellProfiler's <code>run_batch_general</code> script</a></li>
<li class="chapter" data-level="4.3" data-path="run-each-cellprofiler-step.html"><a href="run-each-cellprofiler-step.html#configure-distributed-cellprofilers-fleet-file"><i class="fa fa-check"></i><b>4.3</b> Configure Distributed-CellProfiler's fleet file</a></li>
<li class="chapter" data-level="4.4" data-path="run-each-cellprofiler-step.html"><a href="run-each-cellprofiler-step.html#change-required-parameters-in-distributed-cellprofilers-config-file"><i class="fa fa-check"></i><b>4.4</b> Change required parameters in Distributed-CellProfiler's config file</a></li>
<li class="chapter" data-level="4.5" data-path="run-each-cellprofiler-step.html"><a href="run-each-cellprofiler-step.html#run-each-cellprofiler-step-1"><i class="fa fa-check"></i><b>4.5</b> Run each CellProfiler step</a><ul>
<li class="chapter" data-level="4.5.1" data-path="run-each-cellprofiler-step.html"><a href="run-each-cellprofiler-step.html#optional-z-projection-1"><i class="fa fa-check"></i><b>4.5.1</b> (Optional) Z projection</a></li>
<li class="chapter" data-level="4.5.2" data-path="run-each-cellprofiler-step.html"><a href="run-each-cellprofiler-step.html#optional-qc-1"><i class="fa fa-check"></i><b>4.5.2</b> (Optional) QC</a></li>
<li class="chapter" data-level="4.5.3" data-path="run-each-cellprofiler-step.html"><a href="run-each-cellprofiler-step.html#illumination-correction-1"><i class="fa fa-check"></i><b>4.5.3</b> Illumination Correction</a></li>
<li class="chapter" data-level="4.5.4" data-path="run-each-cellprofiler-step.html"><a href="run-each-cellprofiler-step.html#optional-assay-dev"><i class="fa fa-check"></i><b>4.5.4</b> (Optional) Assay Dev</a></li>
<li class="chapter" data-level="4.5.5" data-path="run-each-cellprofiler-step.html"><a href="run-each-cellprofiler-step.html#analysis-1"><i class="fa fa-check"></i><b>4.5.5</b> Analysis</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="run-each-cellprofiler-step.html"><a href="run-each-cellprofiler-step.html#optional-do-any-post-cellprofiler-steps"><i class="fa fa-check"></i><b>4.6</b> (Optional) Do any post-CellProfiler steps</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="create-profiles.html"><a href="create-profiles.html"><i class="fa fa-check"></i><b>5</b> Create Profiles</a><ul>
<li class="chapter" data-level="5.1" data-path="create-profiles.html"><a href="create-profiles.html#confirm-environment-configuration"><i class="fa fa-check"></i><b>5.1</b> Confirm Environment Configuration</a></li>
<li class="chapter" data-level="5.2" data-path="create-profiles.html"><a href="create-profiles.html#add-a-large-ebs-volume-to-your-machine"><i class="fa fa-check"></i><b>5.2</b> Add a large EBS volume to your machine</a></li>
<li class="chapter" data-level="5.3" data-path="create-profiles.html"><a href="create-profiles.html#create-database-backend"><i class="fa fa-check"></i><b>5.3</b> Create Database Backend</a></li>
<li class="chapter" data-level="5.4" data-path="create-profiles.html"><a href="create-profiles.html#create-metadata-files"><i class="fa fa-check"></i><b>5.4</b> Create Metadata Files</a></li>
<li class="chapter" data-level="5.5" data-path="create-profiles.html"><a href="create-profiles.html#set-up-github"><i class="fa fa-check"></i><b>5.5</b> Set up GitHub</a></li>
<li class="chapter" data-level="5.6" data-path="create-profiles.html"><a href="create-profiles.html#make-profiles"><i class="fa fa-check"></i><b>5.6</b> Make Profiles</a><ul>
<li class="chapter" data-level="5.6.1" data-path="create-profiles.html"><a href="create-profiles.html#optional---set-up-compute-environment"><i class="fa fa-check"></i><b>5.6.1</b> Optional - set up compute environment</a></li>
<li class="chapter" data-level="5.6.2" data-path="create-profiles.html"><a href="create-profiles.html#set-new-environment-variables"><i class="fa fa-check"></i><b>5.6.2</b> Set new environment variables</a></li>
<li class="chapter" data-level="5.6.3" data-path="create-profiles.html"><a href="create-profiles.html#if-a-first-batch-in-this-compute-environment-make-some-directories"><i class="fa fa-check"></i><b>5.6.3</b> If a first batch in this compute environment, make some directories</a></li>
<li class="chapter" data-level="5.6.4" data-path="create-profiles.html"><a href="create-profiles.html#add-your-backend-files"><i class="fa fa-check"></i><b>5.6.4</b> Add your backend files</a></li>
<li class="chapter" data-level="5.6.5" data-path="create-profiles.html"><a href="create-profiles.html#if-a-first-batch-in-this-compute-environment-clone-your-repository"><i class="fa fa-check"></i><b>5.6.5</b> If a first batch in this compute environment, clone your repository</a></li>
<li class="chapter" data-level="5.6.6" data-path="create-profiles.html"><a href="create-profiles.html#if-a-first-batch-in-this-project-weld-the-recipe-into-the-repository"><i class="fa fa-check"></i><b>5.6.6</b> If a first batch in this project, weld the recipe into the repository</a></li>
<li class="chapter" data-level="5.6.7" data-path="create-profiles.html"><a href="create-profiles.html#if-a-first-batch-in-this-compute-environment-set-up-the-environment"><i class="fa fa-check"></i><b>5.6.7</b> If a first batch in this compute environment, set up the environment</a></li>
<li class="chapter" data-level="5.6.8" data-path="create-profiles.html"><a href="create-profiles.html#activate-the-environment"><i class="fa fa-check"></i><b>5.6.8</b> Activate the environment</a></li>
<li class="chapter" data-level="5.6.9" data-path="create-profiles.html"><a href="create-profiles.html#if-a-first-batch-in-this-project-create-the-necessary-directories"><i class="fa fa-check"></i><b>5.6.9</b> If a first batch in this project, create the necessary directories</a></li>
<li class="chapter" data-level="5.6.10" data-path="create-profiles.html"><a href="create-profiles.html#download-the-load_data_csvs"><i class="fa fa-check"></i><b>5.6.10</b> Download the load_data_CSVs</a></li>
<li class="chapter" data-level="5.6.11" data-path="create-profiles.html"><a href="create-profiles.html#download-the-metadata-files"><i class="fa fa-check"></i><b>5.6.11</b> Download the metadata files</a></li>
<li class="chapter" data-level="5.6.12" data-path="create-profiles.html"><a href="create-profiles.html#make-the-config-file"><i class="fa fa-check"></i><b>5.6.12</b> Make the config file</a></li>
<li class="chapter" data-level="5.6.13" data-path="create-profiles.html"><a href="create-profiles.html#set-up-the-profiles"><i class="fa fa-check"></i><b>5.6.13</b> Set up the profiles</a></li>
<li class="chapter" data-level="5.6.14" data-path="create-profiles.html"><a href="create-profiles.html#run-the-profiling-workflow"><i class="fa fa-check"></i><b>5.6.14</b> Run the profiling workflow</a></li>
<li class="chapter" data-level="5.6.15" data-path="create-profiles.html"><a href="create-profiles.html#push-resulting-files-back-up-to-github"><i class="fa fa-check"></i><b>5.6.15</b> Push resulting files back up to GitHub</a></li>
<li class="chapter" data-level="5.6.16" data-path="create-profiles.html"><a href="create-profiles.html#push-resulting-files-up-to-s3"><i class="fa fa-check"></i><b>5.6.16</b> Push resulting files up to S3</a></li>
</ul></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="appendix-a.html"><a href="appendix-a.html"><i class="fa fa-check"></i><b>A</b> Appendix A</a><ul>
<li class="chapter" data-level="A.1" data-path="appendix-a.html"><a href="appendix-a.html#project-folder-structure-guidance"><i class="fa fa-check"></i><b>A.1</b> Project folder structure guidance</a></li>
<li class="chapter" data-level="A.2" data-path="appendix-a.html"><a href="appendix-a.html#directory-structure"><i class="fa fa-check"></i><b>A.2</b> Directory structure</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Image-based Profiling Handbook</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="create-profiles" class="section level1">
<h1><span class="header-section-number">Chapter 5</span> Create Profiles</h1>
<div id="confirm-environment-configuration" class="section level2">
<h2><span class="header-section-number">5.1</span> Confirm Environment Configuration</h2>
<p>You CAN, if you choose, make your backends with the same machine that you have used to make CSVs and run DCP for CellProfiler. However, we typically do not, since backend creation can take a long time so it is desirable to use 2 machines - a small inexpensive machine with only a few CPUs for all the steps before backends and a larger machine with at least as many CPUs as (the number of plates in your batch +1) for backend creation. Both machines can be turned off when not in use and when off will generate only minimal charges.</p>
<p>To make a new backend machine, follow the identical instructions as the in the section below, only with a larger Instance Type (such as an m4.10xlarge).</p>
<ul>
<li><a href="configure-environment-for-full-profiling-pipeline.html#launch-an-aws-virtual-machine-for-making-csvs-and-running-distributed-cellprofiler">Launch an AWS Virtual Machine for making CSVs and running Distributed-CellProfiler</a></li>
</ul>
<p>Since backend creation also typically requires a large amount of hard disk space, which you pay for whether or not the machine is on, we recommend attaching a large EBS volume to your backend creation machine only when needed, then detaching and terminating it when not in use.</p>
</div>
<div id="add-a-large-ebs-volume-to-your-machine" class="section level2">
<h2><span class="header-section-number">5.2</span> Add a large EBS volume to your machine</h2>
<p>Follow the AWS instructions for <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-creating-volume.html">creating</a> and <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-attaching-volume.html">attaching</a> the EBS volume. Two critical factors to note- the volume must be created in the same subnet as your backend creation machine, and should be approximately 2X the size as the analysis files in your batch - this is most easily figured by navigating to that location in the S3 web console and selecting &quot;Actions -&gt; Calculate total size&quot;.</p>
<p>Once the volume is created and attached, ensure the machine is started and SSH connect to it.</p>
<p>Create a temp directory which is required when creating the database backed using <code>cytominer-database</code> (discussed later).</p>
<div class="sourceCode"><pre class="sourceCode sh"><code class="sourceCode bash"><span class="fu">mkdir</span> ~/ebs_tmp</code></pre></div>
<p>Get the name of the disk and attach it.</p>
<div class="sourceCode"><pre class="sourceCode sh"><code class="sourceCode bash"><span class="co"># check the name of the disk</span>
<span class="ex">lsblk</span>

<span class="co">#&gt; NAME    MAJ:MIN RM   SIZE RO TYPE MOUNTPOINT</span>
<span class="co">#&gt; xvda    202:0    0     8G  0 disk</span>
<span class="co">#&gt; └─xvda1 202:1    0     8G  0 part /</span>
<span class="co">#&gt; xvdba    202:80   0   100G  0 disk</span>

<span class="co"># check if it has a file system</span>
<span class="fu">sudo</span> file -s /dev/xvdba
<span class="co"># ...likely not, in which case you get:</span>
<span class="co">#&gt; /dev/xvdf: data</span>

<span class="co"># if no file system, then create it</span>
<span class="fu">sudo</span> mkfs -t ext4 /dev/xvdba

<span class="co"># mount it</span>
<span class="fu">sudo</span> mount /dev/xvdba /home/ubuntu/ebs_tmp/

<span class="co"># change perm</span>
<span class="fu">sudo</span> chmod 777 ~/ebs_tmp/</code></pre></div>
<p>If you are starting from here, make sure the following steps have been completed on your ec2 instance and/or session before proceeding</p>
<ul>
<li><a href="configure-environment-for-full-profiling-pipeline.html#configure-environment-for-full-profiling-pipeline">Configure Environment for Full Profiling Pipeline</a></li>
<li><a href="setup-images.html#create-list-of-plates">Create list of plates</a></li>
</ul>
</div>
<div id="create-database-backend" class="section level2">
<h2><span class="header-section-number">5.3</span> Create Database Backend</h2>
<p>Run creation of sqlite backend as well as aggregation of measurements into per-well profiles. This process can be very slow since the files are read from s3fs/EFS. We recommend first downloading the CSVs files locally on an EBS volume attached to the ec2 instance you are running on, and then ingesting.</p>
<p>To do so, first recreate the analysis output folder structure on the EBS volume:</p>
<div class="sourceCode"><pre class="sourceCode sh"><code class="sourceCode bash"><span class="fu">mkdir</span> -p ~/ebs_tmp/<span class="va">${PROJECT_NAME}</span>/workspace/software

<span class="bu">cd</span> ~/ebs_tmp/<span class="va">${PROJECT_NAME}</span>/workspace/software

<span class="kw">if</span><span class="bu"> [</span> <span class="ot">-d</span> pycytominer<span class="bu"> ]</span>; <span class="kw">then</span> <span class="fu">rm</span> -rf pycytominer<span class="kw">;</span> <span class="kw">fi</span>

<span class="fu">git</span> clone https://github.com/cytomining/pycytominer.git

<span class="bu">cd</span> pycytominer

<span class="fu">git</span> checkout jump

<span class="ex">python3</span> -m pip install -e .</code></pre></div>
<p>The command below first calls <code>cytominer-database ingest</code> to create the SQLite backend, and then pycytominer's <code>aggregate_profiles</code> to create per-well profiles. Once complete, all files are uploaded to S3 and the local cache are deleted. This step takes several hours, but metadata creation and GitHub setup can be done in this time.</p>
<p><a href="https://github.com/cytomining/pycytominer/blob/jump/pycytominer/cyto_utils/collate.py">collate.py</a> ingests and indexes the database.</p>
<div class="sourceCode"><pre class="sourceCode sh"><code class="sourceCode bash"><span class="ex">pyenv</span> shell 3.8.10

<span class="fu">mkdir</span> -p  ../../log/<span class="va">${BATCH_ID}</span>/
<span class="ex">parallel</span> \
--max-procs <span class="va">${MAXPROCS}</span> \
--ungroup \
--eta \
--joblog ../../log/<span class="va">${BATCH_ID}</span>/collate.log \
--results ../../log/<span class="va">${BATCH_ID}</span>/collate \
--files \
--keep-order \
python3 pycytominer/cyto_utils/collate.py <span class="va">${BATCH_ID}</span>  pycytominer/cyto_utils/ingest_config.ini <span class="dt">{1}</span> \
--temp ~/ebs_tmp \
--remote=s3://<span class="va">${BUCKET}</span>/projects/<span class="va">${PROJECT_NAME}</span>/workspace :::: <span class="va">${PLATES}</span></code></pre></div>

<div class="rmdnote">
<code>collate.py</code> does not recreate the SQLite backend if it already exists in the local cache. Add <code>--overwrite</code> flag to recreate.
</div>


<div class="rmdnote">
For pipelines that use FlagImage to skip the measurements modules if the image failed QC, the failed images will have Image.csv files with fewer columns that the rest (because columns corresponding to aggregated measurements will be absent). The ingest command will show a warning related to sqlite: <code>expected X columns but found Y - filling the rest with NULL</code>. This is expected behavior.
</div>


<div class="rmdnote">
There is a known <a href="https://github.com/cytomining/cytominer-database/issues/100">issue</a> where if the alphabetically-first CSV failed QC in a pipeline where &quot;Skip image if flagged&quot; is turned on, the databases will not be created. We are working to fix this, but in the meantime we recommend either not skipping processing of your flagged images (and removing them from your data downstream) or deleting the alphabetically-first CSVs until you come to one where the pipeline ran completely.
</div>

<p>This is the resulting structure of <code>backend</code> on S3 (one level below <code>workspace</code>) for <code>SQ00015167</code>:</p>
<pre><code>└── backend
    └── 2016_04_01_a549_48hr_batch1
        └── SQ00015167
            ├── SQ00015167.csv
            └── SQ00015167.sqlite</code></pre>
<p>At this point, the user needs to use the <a href="https://github.com/cytomining/profiling-template">profiling template</a> to use <a href="https://github.com/cytomining/pycytominer/">pycytominer</a> to annotate the profiles with metadata, normalize them, and feature select them.</p>
</div>
<div id="create-metadata-files" class="section level2">
<h2><span class="header-section-number">5.4</span> Create Metadata Files</h2>
<p>First, get metadata for the plates. This should be created beforehand and uploaded into S3.</p>
<p>This is the structure of the metadata folder (one level below <code>workspace</code>):</p>
<pre><code>└── metadata
    └── platemaps
        └── 2016_04_01_a549_48hr_batch1
            ├── barcode_platemap.csv
            └── platemap
                └── C-7161-01-LM6-006.txt</code></pre>
<p><code>2016_04_01_a549_48hr_batch1</code> is the batch name – the plates (and all related data) are arranged under batches, as seen below.</p>
<p><code>barcode_platemap.csv</code> is structured as shown below. <code>Assay_Plate_Barcode</code> and <code>Plate_Map_Name</code> are currently the only mandatory columns (they are used to join the metadata of the plate map with each assay plate). Each unique entry in the <code>Plate_Map_Name</code> should have a corresponding tab-separated file <code>.txt</code> file under <code>platemap</code> (e.g. <code>C-7161-01-LM6-006.txt</code>).</p>
<pre><code>Assay_Plate_Barcode,Plate_Map_Name
SQ00015167,C-7161-01-LM6-006</code></pre>
<p>The tab-separated files are plate maps and are structured like this: (This is the typical format followed by Broad Chemical Biology Platform)</p>
<pre><code>plate_map_name  well_position broad_sample  mg_per_ml mmoles_per_liter  solvent
C-7161-01-LM6-006 A07 BRD-K18895904-001-16-1  3.12432000000000016 9.99999999999999999 DMSO
C-7161-01-LM6-006 A08 BRD-K18895904-001-16-1  1.04143999999919895 3.33333333333076923 DMSO
C-7161-01-LM6-006 A09 BRD-K18895904-001-16-1  0.347146666668001866  1.11111111111538462 DMSO</code></pre>

<div class="rmdnote">
<ul>
<li><code>plate_map_name</code> should be identical to the name of the file (without extension).</li>
<li><code>plate_map_name</code> and <code>well_position</code> are currently the only mandatory columns.
</div>
</li>
</ul>
<p>The external metadata is an optional file tab separated <code>.tsv</code> file that contains the mapping between a perturbation identifier to other metadata. The name of the perturbation identifier column (e.g. <code>broad_sample</code>) should be the same as the column in platemap.txt.</p>
<p>The external metadata file should be placed in a folder named <code>external_metadata</code> within the <code>metadata</code> folder. If this file is provided, then the following should be the folder structure</p>
<pre><code>└── metadata
    ├── external_metadata
    │   └── external_metadata.tsv
    └── platemaps
        └── 2016_04_01_a549_48hr_batch1
            ├── barcode_platemap.csv
            └── platemap
                └── C-7161-01-LM6-006.txt</code></pre>
</div>
<div id="set-up-github" class="section level2">
<h2><span class="header-section-number">5.5</span> Set up GitHub</h2>
<p>Once and only once - fork the <a href="https://github.com/cytomining/profiling-recipe">profiling recipe</a> to your own user name (Each time you make a new project - you may want to <a href="https://docs.github.com/en/github/collaborating-with-pull-requests/working-with-forks/syncing-a-fork">keep your fork up to date</a></p>
<p>Once per new PROJECT, not new batch - make a copy of the <a href="https://github.com/cytomining/profiling-template">template repository</a> into your preferred organization with a project name that is similar OR identical to its project tag on S3 and elsewhere.</p>
</div>
<div id="make-profiles" class="section level2">
<h2><span class="header-section-number">5.6</span> Make Profiles</h2>
<div id="optional---set-up-compute-environment" class="section level3">
<h3><span class="header-section-number">5.6.1</span> Optional - set up compute environment</h3>
<p>These final steps are small and can be done either in your local environment or on your node used to build the backends. Conda and Git LFS are currently required. For now, the backend creation VMs don’t contain conda or git lfs, but in the meantime these are the commands used to install both for a Linux ecosystem (otherwise, search for your own OS).</p>
<pre><code>wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda.sh
bash ~/miniconda.sh -b -p $HOME/miniconda
export PATH=&quot;/home/ubuntu/miniconda/bin:$PATH&quot;
source .bashrc
conda init bash
source .bashrc

curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | sudo bash</code></pre>
<p>If not using the same machine + tmux as for making backends, where your environment variables are already set, set them up</p>
<ul>
<li><a href="configure-environment-for-full-profiling-pipeline.html#configure-environment-for-full-profiling-pipeline">Configure Environment for Full Profiling Pipeline</a></li>
</ul>
</div>
<div id="set-new-environment-variables" class="section level3">
<h3><span class="header-section-number">5.6.2</span> Set new environment variables</h3>
<p>Specifically, <code>ORG</code> and <code>DATA</code> should be the GitHub organization and repository name used when creating the data repository from the template. <code>USER</code> should be your GitHub username. CONFIG_FILE will be the name of the config file used for this run, so something that makes it distinguishable (ie, batch numbers being run at this time) is helpful.</p>
<pre><code>ORG=broadinstitute
DATA=2015_10_05_DrugRepurposing_AravindSubramanian_GolubLab_Broad
USER=gh_username
CONFIG_FILE=config_batch1</code></pre>
</div>
<div id="if-a-first-batch-in-this-compute-environment-make-some-directories" class="section level3">
<h3><span class="header-section-number">5.6.3</span> If a first batch in this compute environment, make some directories</h3>
<pre><code>mkdir -p ~/work/projects/${PROJECT_NAME}/workspace/{backend,software}</code></pre>
</div>
<div id="add-your-backend-files" class="section level3">
<h3><span class="header-section-number">5.6.4</span> Add your backend files</h3>
<pre><code>aws s3 sync s3://${BUCKET}/projects/${PROJECT_NAME}/workspace/backend/${BATCH_ID} ~/work/projects/${PROJECT_NAME}/workspace/backend/${BATCH_ID} --exclude=&quot;*&quot; --include=&quot;*.csv&quot;</code></pre>
</div>
<div id="if-a-first-batch-in-this-compute-environment-clone-your-repository" class="section level3">
<h3><span class="header-section-number">5.6.5</span> If a first batch in this compute environment, clone your repository</h3>
<pre><code>cd ~/work/projects/${PROJECT_NAME}/workspace/software
git clone git@github.com:${ORG}/${DATA}.git
#depending on your repo/machine set up you may need to provide credentials here
cd ${DATA}</code></pre>
</div>
<div id="if-a-first-batch-in-this-project-weld-the-recipe-into-the-repository" class="section level3">
<h3><span class="header-section-number">5.6.6</span> If a first batch in this project, weld the recipe into the repository</h3>
<pre><code>git submodule add https://github.com/${USER}/profiling-recipe.git profiling-recipe
git add profiling-recipe
git add .gitmodules
git commit -m &#39;finalizing the recipe weld&#39;
git push
#depending on your repo/machine set up you may need to provide credentials here
git submodule update --init --recursive</code></pre>
</div>
<div id="if-a-first-batch-in-this-compute-environment-set-up-the-environment" class="section level3">
<h3><span class="header-section-number">5.6.7</span> If a first batch in this compute environment, set up the environment</h3>
<pre><code>cp profiling-recipe/environment.yml .
conda env create --force --file environment.yml</code></pre>
</div>
<div id="activate-the-environment" class="section level3">
<h3><span class="header-section-number">5.6.8</span> Activate the environment</h3>
<pre><code>conda activate profiling</code></pre>
</div>
<div id="if-a-first-batch-in-this-project-create-the-necessary-directories" class="section level3">
<h3><span class="header-section-number">5.6.9</span> If a first batch in this project, create the necessary directories</h3>
<pre><code>profiling-recipe/scripts/create_dirs.sh</code></pre>
</div>
<div id="download-the-load_data_csvs" class="section level3">
<h3><span class="header-section-number">5.6.10</span> Download the load_data_CSVs</h3>
<pre><code>aws s3 sync s3://${BUCKET}/projects/${PROJECT_NAME}/workspace/load_data_csv/${BATCH_ID} load_data_csv/${BATCH_ID}
gzip -r  load_data_csv/${BATCH_ID}</code></pre>
</div>
<div id="download-the-metadata-files" class="section level3">
<h3><span class="header-section-number">5.6.11</span> Download the metadata files</h3>
<pre><code>aws s3 sync s3://${BUCKET}/projects/${PROJECT_NAME}/workspace/metadata/${BATCH_ID} metadata/platemaps/${BATCH_ID}</code></pre>
</div>
<div id="make-the-config-file" class="section level3">
<h3><span class="header-section-number">5.6.12</span> Make the config file</h3>
<pre><code>cp profiling-recipe/config_template.yml config_files/${CONFIG_FILE}.yml
nano config_files/${CONFIG_FILE}.yml</code></pre>

<div class="rmdnote">
<p>The changes you will likely need to make for most small use cases following this handbook- for <code>aggregate</code> set <code>perform</code> to <code>false</code>, for <code>annotate</code> sub-setting <code>external</code> set <code>perform</code> to <code>false</code>, in <code>feature_select</code> set <code>gct</code> to <code>true</code>, and finally at the bottom set the batch(es) and plates names</p>
For large batches with many DMSO wells and external metadata ala the JUMP project - set <code>perform</code> under <code>external</code> to <code>true</code>, set <code>file</code> to the name of the external metadata file and set <code>merge_column</code> to the name of the compound identifier column in platemap.txt and external_metadata.tsv.
</div>

</div>
<div id="set-up-the-profiles" class="section level3">
<h3><span class="header-section-number">5.6.13</span> Set up the profiles</h3>
<p>Note that the “find” step can take a few seconds/minutes</p>
<pre><code>mkdir -p profiles/${BATCH_ID}
find ../../backend/${BATCH_ID}/ -type f -name &quot;*.csv&quot; -exec profiling-recipe/scripts/csv2gz.py {} \;
rsync -arzv --include=&quot;*/&quot; --include=&quot;*.gz&quot; --exclude &quot;*&quot; ../../backend/${BATCH_ID}/ profiles/${BATCH_ID}/</code></pre>
</div>
<div id="run-the-profiling-workflow" class="section level3">
<h3><span class="header-section-number">5.6.14</span> Run the profiling workflow</h3>
<p>Especially for large number of plates, this will take some time. Output will be logged to the console as different steps proceed.</p>
<pre><code>python profiling-recipe/profiles/profiling_pipeline.py  --config config_files/{$CONFIG_FILE}.yml</code></pre>
</div>
<div id="push-resulting-files-back-up-to-github" class="section level3">
<h3><span class="header-section-number">5.6.15</span> Push resulting files back up to GitHub</h3>
<pre><code>git add *
git commit -m &#39;add profiles for batch _&#39;
git push</code></pre>
</div>
<div id="push-resulting-files-up-to-s3" class="section level3">
<h3><span class="header-section-number">5.6.16</span> Push resulting files up to S3</h3>
<pre><code>parallel aws s3 sync {1} s3://${BUCKET}/projects/${PROJECT_NAME}/workspace/{1} ::: config_files gct profiles quality_control</code></pre>

</div>
</div>
</div>



            </section>

          </div>
        </div>
      </div>
<a href="run-each-cellprofiler-step.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="appendix-a.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/cytomining/profiling-handbook/edit/master/05-create-profiles.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["bookdown-demo.pdf", "bookdown-demo.epub"],
"search": {
"engine": "lunr",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

</body>

</html>
