[["index.html", "Image-based Profiling Handbook Preface", " Image-based Profiling Handbook Tim Becker, Beth Cimini, Shantanu Singh, Gregory Way, Hamdah Abbasi 2020-09-01 Preface "],["configure-environment-for-full-profiling-pipeline.html", "Chapter 1 Configure Environment for Full Profiling Pipeline 1.1 Launch an AWS Virtual Machine 1.2 Define Environment Variables 1.3 Create Directories", " Chapter 1 Configure Environment for Full Profiling Pipeline 1.1 Launch an AWS Virtual Machine Launch an EC2 node using AMI cytomining/images/hvm-ssd/cytominer-ubuntu-trusty-14.04-amd64-server-*, created using https://github.com/cytomining/cytominer-vm. Note that the region must be set to US East (N. Virginia) (us-east-1a or us-east-1b). To use a different region, modify the vpc module in the AWS infrastructure setup. You will need to create an AMI for your own infrastructure because the provisioning includes mounting S3 and EFS, which is account specific. We recommend using an m4.xlarge instance, with a 110Gb EBS volume (roughly, EBS size should be ((number of cores -1) * 30)+20). Note: Proper configuration is essential to mount the S3 bucket. The following configuration provides an example, named imaging_platform (modifications will be necessary). Launch an ec2 instance on AWS AMI: cytomining/images/hvm-ssd/cytominer-ubuntu-trusty-14.04-amd64-server-1529668435 Instance Type: m4.xlarge Network: vpc-35149752 - Subnet: Default (imaging platform terraform) IAM role: s3-imaging-platform-role Add New Volume (if necessary): EBS with 110 GiB No Tags Select Existing Security Group: SSH_HTTP Review and Launch ssh -i &lt;USER&gt;.pem ubuntu@&lt;Public DNS IPv4&gt; After starting the instance, ensure that the S3 bucket is mounted on ~/bucket. If not, run sudo mount -a. Note that given this configuration in this AMI, EFS can only be mounted from us-east-1a or us-east-1b. This can be changed by appropriately editing the EFS configuration via Terraform. Log in to the EC2 instance. Check available space on the instance df -h Ensure that the Available column is at least 30Gb x p, where p is the number of plates you will process in parallel when creating the database backend. We recommend p to be one less than the number of cores (p = 3 for m4.xlarge, so 60Gb should be available). If you don't have sufficient space, mount an EBS volume (below). Enter your AWS credentials aws configure The infrastructure is configured with one S3 bucket. Mount this S3 bucket (if it is not automatically mounted) sudo mount -a Check that the bucket was was mounted. This path should exist: ls ~/bucket/projects 1.2 Define Environment Variables These variables will be used throughout the project to tag instances, logs etc so that you know which machines are working on what, which files to operate on, where your logs are, etc. PROJECT_NAME=2015_10_05_DrugRepurposing_AravindSubramanian_GolubLab_Broad BATCH_ID=2016_04_01_a549_48hr_batch1 BUCKET=imaging-platform MAXPROCS=3 # m4.xlarge has 4 cores; this should be # of cores on your instance - 1 1.3 Create Directories See note above about EFS - that it can only be mounted from us-east-1a or 1b. mkdir -p ~/efs/${PROJECT_NAME}/workspace/ cd ~/efs/${PROJECT_NAME}/workspace/ mkdir -p log/${BATCH_ID} Create a temp directory which is required when creating the database backed using cytominer-database (discussed later). This is also useful if you decide to run CellProfiler directly on this node – running the Cell Painting analysis pipeline results in large temporary files. mkdir ~/ebs_tmp If at this point you realize that the ec2 instance does not have enough space (which you can check using du -hs), create and attach an EBS volume, and then mount it. Make sure that the EBS is created in the same region and availability zone as the ec2 instance. For more information about mounting the EBS see https://devopscube.com/mount-ebs-volume-ec2-instance/. # check the name of the disk lsblk #&gt; NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT #&gt; xvda 202:0 0 8G 0 disk #&gt; └─xvda1 202:1 0 8G 0 part / #&gt; xvdba 202:80 0 100G 0 disk # check if it has a file system sudo file -s /dev/xvdba # ...likely not, in which case you get: #&gt; /dev/xvdf: data # if no file system, then create it sudo mkfs -t ext4 /dev/xvdba # mount it sudo mount /dev/xvdba /home/ubuntu/ebs_tmp/ # change perm sudo chmod 777 ~/ebs_tmp/ "],["configure-tools-to-process-images.html", "Chapter 2 Configure Tools to Process Images 2.1 Download Software 2.2 Update some packages 2.3 Setup Distributed CellProfiler", " Chapter 2 Configure Tools to Process Images 2.1 Download Software You may need to authenticate your GitHub account to be able to clone these: (Note that cellpainting_scripts is currently a private repository; contact our team for access) cd ~/efs/${PROJECT_NAME}/workspace/ mkdir software cd software git clone git@github.com:broadinstitute/cellpainting_scripts.git git clone git@github.com:broadinstitute/pe2loaddata.git git clone git@github.com:broadinstitute/cytominer_scripts.git git clone git@github.com:CellProfiler/Distributed-CellProfiler.git cd .. To authenticate your GitHub account, follow instructions to setup SSH keys here https://help.github.com/articles/connecting-to-github-with-ssh/. You can also clone these repos by following the structure below and inputting your OAuth token as necessary. git clone https://username@github.com/broadinstitute/cytominer_scripts.git If these repos have already been cloned, git pull to make sure they are up to date. This is the resulting structure of software on EFS (one level below workspace): └── software ├── Distributed-CellProfiler ├── cellpainting_scripts ├── cytominer_scripts └── pe2loaddata 2.2 Update some packages The AWS Virtual Machine has several required packages preinstalled. However, the ones listed below need to be updated. 2.2.1 cytominer-database pyenv shell 3.5.1 pip install --upgrade cytominer-database` 2.3 Setup Distributed CellProfiler cd ~/efs/${PROJECT_NAME}/workspace/software/Distributed-CellProfiler/ pyenv shell 2.7.12 pip install -r files/requirements.txt "],["setup-pipelines-and-images.html", "Chapter 3 Setup Pipelines and Images 3.1 Get CellProfiler Pipelines 3.2 Specify Pipeline Set 3.3 Upload Images 3.4 Prepare Images 3.5 Create List of Plates 3.6 Create LoadData CSVs", " Chapter 3 Setup Pipelines and Images 3.1 Get CellProfiler Pipelines Cell Painting pipelines are stored in a GitHub repo. If you are using a new pipeline, be sure to add it to the repo first. Follow instructions on https://github.com/broadinstitute/imaging-platform-pipelines for adding new pipelines. cd ~/efs/${PROJECT_NAME}/workspace/ mkdir github cd github/ git clone https://github.com/broadinstitute/imaging-platform-pipelines.git cd .. ln -s github/imaging-platform-pipelines pipelines This is the resulting structure of github and pipelines on EFS (one level below workspace): ├── github │    └── imaging-platform-pipelines └── pipelines -&gt; github/imaging-platform-pipelines 3.2 Specify Pipeline Set PIPELINE_SET=cellpainting_a549_20x_with_bf_phenix_bin1 Ensure that, both, analysis.cppipe as well as illum.cppipe are present for this set. As well, each pipeline should have a _without_batchfile version of it in the same directory. It's easy to create such a version of the pipeline - simply copy it and set enabled=False for the CreateBatchFiles module (like here). 3.3 Upload Images Your image files should be uploaded to AWS from your local compute environment via a tool like Cyberduck or the AWS CLI (aws s3 sync /local/path s3://BUCKET/PROJECT_NAME/BATCH_ID/images) (see also Appendix A.2 for more information on folder structures). Some important tips BEFORE uploading (these are much more difficult to fix once uploaded): Ensure your image sets are complete i.e. all image sets should have the same number of channels and z-planes, and that this is true across the entire batch of plates you are processing. Avoid folder names with spaces Plate names should not have leading 0's (ie 123 not 000123) VERY IMPORTANT- If using pe2loaddata (described later) to generate your image CSVs, please ensure the folder name contains the plate name given when imaging on the Phenix microscope (can be checked in the Index.idx.xml) 3.4 Prepare Images (if using pe2loaddata to create image sets) Create soft link to the image folder. Note that the relevant S3 bucket has been mounted at /home/ubuntu/bucket/. The folder structure for images differs between S3 and EFS. This can be potentially confusing. However note that the step below simply creates a soft link to the images in S3; no files are copied. Further, when pe2loaddata is run (later in the process, via create_csv_from_xml.sh) it resolves the soft link, so the the resulting LoadData CSV files end up having the paths to the images as they exist on S3. Thus the step below (of creating a softlink) only serves the purpose of making the images folder have a similar structure as the others (e.g. load_data_csv, metadata, analysis). cd ~/efs/${PROJECT_NAME}/workspace/ mkdir images cd images ln -s ~/bucket/projects/${PROJECT_NAME}/${BATCH_ID}/images/ ${BATCH_ID} cd .. This is the resulting structure of the image folder on EFS (one level below workspace): └── images    └── 2016_04_01_a549_48hr_batch1 -&gt; /home/ubuntu/bucket/projects/2015_10_05_DrugRepurposing_AravindSubramanian_GolubLab_Broad/2016_04_01_a549_48hr_batch1/images/ This is the structure of the image folder on S3 (one level above workspace, under the folder 2016_04_01_a549_48hr_batch1.) Here, only one plate (SQ00015167__2016-04-21T03_34_00-Measurement1) is show but there are often many more. └── images    └── 2016_04_01_a549_48hr_batch1 └── SQ00015167__2016-04-21T03_34_00-Measurement1    ├── Assaylayout    ├── FFC_Profile    └── Images ├── r01c01f01p01-ch1sk1fk1fl1.tiff ├── r01c01f01p01-ch2sk1fk1fl1.tiff ├── r01c01f01p01-ch3sk1fk1fl1.tiff ├── r01c01f01p01-ch4sk1fk1fl1.tiff └── r01c01f01p01-ch5sk1fk1fl1.tiff SQ00015167__2016-04-21T03_34_00-Measurement1 is the typical nomenclature followed by Broad Chemical Biology Platform for plate names. Measurement1 indicates the first attempt to image the plate. Measurement2 indicates second attempt and so on. Ensure that there's only one folder corresponding to a plate before running create_csv_from_xml.sh below (it gracefully exits if not). 3.5 Create List of Plates Create a text file with one plate id per line. The plate IDs, if using pe2loaddata, must match the plate IDs given when operating the Phenix. Otherwise, they should match CellProfiler's understanding of the Plate grouping variable, whether that is explicitly stated in a loaddata CSV OR produced from the Metadata module if the CSVs and/or batch files are created using CellProfiler's input modules. For downstream purposes, i.e. cytominer, you may choose to use only so much of the plate name as you need to keep the plates unique (e.g. SQ00015167 instead of SQ00015167__2016-04-21T03_34_00-Measurement1 to keep the names compact. mkdir -p ~/efs/${PROJECT_NAME}/workspace/scratch/${BATCH_ID}/ PLATES=$(readlink -f ~/efs/${PROJECT_NAME}/workspace/scratch/${BATCH_ID}/plates_to_process.txt) Option 1: create the list echo &quot;SQ00015130 SQ00015168 SQ00015167 SQ00015166 SQ00015165&quot;|tr &quot; &quot; &quot;\\n&quot; &gt; ${PLATES} Option 2: create the list automatically, using some pattern to create the shortened name of the plate (e.g. SQ00015130 is the first substring of SQ00015167__2016-04-21T03_34_00-Measurement1 when split by _). This creates a list of all plates in the batch: ls ~/efs/${PROJECT_NAME}/workspace/images/${BATCH_ID}/ | cut -d &#39;_&#39; -f 1 &gt;&gt; $PLATES We'll also set the SAMPLE_PLATE_ID and SAMPLE_FULL_PLATE_NAME variables, which are used in the profiling steps when a single plate name is required SAMPLE_PLATE_ID=SQ00015130 SAMPLE_FULL_PLATE_NAME=SQ0015130__2019-08-30T18_45_25-Measurement1 3.6 Create LoadData CSVs The script below works only for Phenix microscopes – it reads a standard XML file (Index.idx.xml) and writes a LoadData csv file. For other microscopes, you will have to roll your own (see Appendix B). The script below requires config.yml, which specifies (1) the mapping between channel names in Index.idx.xml and the channel names in the CellProfiler pipelines and (2) metadata to extract from Index.idx.xml. Here's a truncated sample config.yml (see the repository for the full file) channels: HOECHST 33342: OrigDNA Alexa 568: OrigAGP Alexa 647: OrigMito Alexa 488: OrigER 488 long: OrigRNA Brightfieldlow: OrigBrightfield metadata: Row: Row Col: Col FieldID: FieldID PlaneID: PlaneID ChannelID: ChannelID ChannelName: ChannelName ImageResolutionX: ImageResolutionX [...] Often, the values of the keys for channels are different in Index.idx.xml, so for example, above, we have Brightfieldlow: OrigBrightfield but the keys for channels could be different in Index.idx.xml: $ tail -n 500 ~/efs/${PROJECT_NAME}/workspace/images/${BATCH_ID}/${SAMPLE_FULL_PLATE_NAME}/Images/Index.idx.xml|grep ChannelName|sort -u &lt;ChannelName&gt;488 long&lt;/ChannelName&gt; &lt;ChannelName&gt;Alexa 488&lt;/ChannelName&gt; &lt;ChannelName&gt;Alexa 568&lt;/ChannelName&gt; &lt;ChannelName&gt;Alexa 647&lt;/ChannelName&gt; &lt;ChannelName&gt;Brightfield CP&lt;/ChannelName&gt; &lt;ChannelName&gt;HOECHST 33342&lt;/ChannelName&gt; The brightfield channel is tagged Brightfield CP in Index.idx.xml. Fix this discrepancy by editing config.yml: HOECHST 33342: OrigDNA Alexa 568: OrigAGP Alexa 647: OrigMito Alexa 488: OrigER 488 long: OrigRNA Brightfield CP: OrigBrightfield Ensure that all the metadata fields defined in config.yml are present in the Index.idx.xml. Ensure that the channel names are the same in config.yml and Index.idx.xml Ensure that the LoadData csv files don't already exist; if they do, delete them. The max-procs option is set as 1 because pe2loaddata accesses the image files on s3fs, which doesn't handle multiple requests well. cd ~/efs/${PROJECT_NAME}/workspace/software/cellpainting_scripts/ pyenv shell 2.7.12 parallel \\ --max-procs 1 \\ --eta \\ --joblog ../../log/${BATCH_ID}/create_csv_from_xml.log \\ --results ../../log/${BATCH_ID}/create_csv_from_xml \\ --files \\ --keep-order \\ ./create_csv_from_xml.sh \\ -b ${BATCH_ID} \\ --plate {1} :::: ${PLATES} cd ../../ This is the resulting structure of load_data_csv on EFS (one level below workspace). Files for only SQ00015167 are shown. └── load_data_csv    └── 2016_04_01_a549_48hr_batch1    └── SQ00015167    ├── load_data.csv    └── load_data_with_illum.csv load_data.csv will be used by illum.cppipe and, optionally, qc.cppipe. load_data_with_illum.csv will be used by analysis.cppipe. When creating load_data_with_illum.csv, the script assumes a specific location for the folder containing the illumination correction files. Copy the load data files to S3: aws s3 sync \\ ../../load_data_csv/${BATCH_ID}/ \\ s3://${BUCKET}/projects/${PROJECT_NAME}/workspace/load_data_csv/${BATCH_ID}/ "],["setup-jobs.html", "Chapter 4 Setup Jobs 4.1 Illumination Correction 4.2 Quality Control 4.3 Analysis", " Chapter 4 Setup Jobs For each of the pipelines, we create the following: a CellProfiler batchfile a job file for running using Distributed-CellProfiler a docker commands file that can be run directly on the EC2 node 4.1 Illumination Correction cd ~/efs/${PROJECT_NAME}/workspace/software/cellpainting_scripts/ pyenv shell 3.5.1 TMPDIR=/tmp # change this to ~/ebs_tmp if CellProfiler will be run directly on this node. parallel \\ --max-procs ${MAXPROCS} \\ --eta \\ --joblog ../../log/${BATCH_ID}/create_batch_files_illum.log \\ --results ../../log/${BATCH_ID}/create_batch_files_illum \\ --files \\ --keep-order \\ ./create_batch_files.sh \\ -b ${BATCH_ID} \\ --plate {1} \\ --datafile_filename load_data.csv \\ --output_dir ../../../${BATCH_ID}/illum \\ --pipeline ../../pipelines/${PIPELINE_SET}/illum.cppipe \\ --cp_docker_image cellprofiler/cellprofiler:2.3.1 \\ --create_dcp_config \\ --s3_bucket ${BUCKET} \\ --tmpdir ${TMPDIR} :::: ${PLATES} cd ../../ This is the resulting structure of batchfiles on EFS (one level below workspace). Files for only SQ00015167 are shown. └── batchfiles/ └── 2016_04_01_a549_48hr_batch1 └── SQ00015167 └── illum ├── Batch_data.h5 ├── dcp_config.json ├── cp_docker_commands.txt └── cpgroups.csv dcp_config.json is the job file you will need later for running Distributed-CellProfiler. Here it is for SQ00015167: { &quot;pipeline&quot;: &quot;projects/2015_10_05_DrugRepurposing_AravindSubramanian_GolubLab_Broad/workspace/github/imaging-platform-pipelines/cellpainting_a549_20x_phenix_bin1/illum_without_batchfile.cppipe&quot;, &quot;data_file&quot;: &quot;projects/2015_10_05_DrugRepurposing_AravindSubramanian_GolubLab_Broad/workspace/load_data_csv/2016_04_01_a549_48hr_batch1/SQ00015167/load_data.csv&quot;, &quot;input&quot;: &quot;dummy&quot;, &quot;output&quot;: &quot;/home/ubuntu/bucket/projects/2015_10_05_DrugRepurposing_AravindSubramanian_GolubLab_Broad/2016_04_01_a549_48hr_batch1/illum/SQ00015167&quot;, &quot;groups&quot;: [ { &quot;Metadata&quot;: &quot;Metadata_Plate=SQ00015167&quot; } ] } Check the pipeline and data_file variables in the dcp_config.json file to ensure that they look right. They should be assigned relative paths e.g. projects/XXX/XXX/..., not absolute paths. This can happen if the PIPELINE_SET variable is not set correctly. cp_docker_commands.txt contains the command to compute the illumination functions on the current EC2 instance. The content of cp_docker_commands.txt for SQ00015167 is: docker run \\ --rm \\ --volume=/home/ubuntu/efs/2015_10_05_DrugRepurposing_AravindSubramanian_GolubLab_Broad/workspace/github/imaging-platform-pipelines/cellpainting_20x_phenix_bin1:/pipeline_dir \\ --volume=/2016_04_01_a549_48hr_batch1/SQ00015167:/filelist_dir \\ --volume=/home/ubuntu/efs/2015_10_05_DrugRepurposing_AravindSubramanian_GolubLab_Broad/workspace/load_data_csv/2016_04_01_a549_48hr_batch1/SQ00015167:/datafile_dir \\ --volume=/home/ubuntu/efs/2015_10_05_DrugRepurposing_AravindSubramanian_GolubLab_Broad/workspace/batchfiles/2016_04_01_a549_48hr_batch1/SQ00015167/illum:/batchfile_dir \\ --volume=/home/ubuntu/efs/2015_10_05_DrugRepurposing_AravindSubramanian_GolubLab_Broad/2016_04_01_a549_48hr_batch1/illum/SQ00015167:/output_dir \\ --volume=/tmp:/tmp_dir \\ --volume=/home/ubuntu/efs/2015_10_05_DrugRepurposing_AravindSubramanian_GolubLab_Broad/workspace/status/2016_04_01_a549_48hr_batch1/SQ00015167/illum:/status_dir \\ --volume=/home/ubuntu/bucket/:/home/ubuntu/bucket/ \\ --log-driver=awslogs \\ --log-opt awslogs-group=SQ00015167 \\ --log-opt awslogs-stream=SQ00015167 \\ cellprofiler/cellprofiler:2.3.1 \\ -p /batchfile_dir/Batch_data.h5 \\ -g Metadata_Plate=SQ00015167 \\ --data-file=/datafile_dir/load_data.csv \\ -o /output_dir \\ -t /tmp_dir \\ -d /status_dir/SQ00015167.txt 4.2 Quality Control 4.3 Analysis cd ~/efs/${PROJECT_NAME}/workspace/software/cellpainting_scripts/ pyenv shell 3.5.1 TMPDIR=/tmp # change this to ~/ebs_tmp if CellProfiler will be run directly on this node. parallel \\ --max-procs ${MAXPROCS} \\ --eta \\ --joblog ../../log/${BATCH_ID}/create_batch_files_analysis.log \\ --results ../../log/${BATCH_ID}/create_batch_files_analysis \\ --files \\ --keep-order \\ ./create_batch_files.sh -b ${BATCH_ID} \\ --plate {1} \\ --datafile_filename load_data_with_illum.csv \\ --pipeline ../../pipelines/${PIPELINE_SET}/analysis.cppipe \\ --cp_docker_image cellprofiler/cellprofiler:2.3.1 \\ --create_dcp_config \\ --s3_bucket ${BUCKET} \\ --tmpdir ${TMPDIR} :::: ${PLATES} cd ../../ This is the resulting structure of batchfiles on EFS (one level below workspace). Files for only SQ00015167 are shown. └── batchfiles/ └── 2016_04_01_a549_48hr_batch1 └── SQ00015167 └── analysis ├── Batch_data.h5 ├── dcp_config.json ├── cp_docker_commands.txt └── cpgroups.csv dcp_config.json is the job file you will need later for running Distributed-CellProfiler. Here it is for SQ00015167: { &quot;pipeline&quot;: &quot;projects/2015_10_05_DrugRepurposing_AravindSubramanian_GolubLab_Broad/workspace/github/imaging-platform-pipelines/cellpainting_a549_20x_phenix_bin1/analysis_without_batchfile.cppipe&quot;, &quot;data_file&quot;: &quot;projects/2015_10_05_DrugRepurposing_AravindSubramanian_GolubLab_Broad/workspace/load_data_csv/2016_04_01_a549_48hr_batch1/SQ00015167/load_data_with_illum.csv&quot;, &quot;input&quot;: &quot;dummy&quot;, &quot;output&quot;: &quot;/home/ubuntu/bucket/projects/2015_10_05_DrugRepurposing_AravindSubramanian_GolubLab_Broad/workspace/analysis/2016_04_01_a549_48hr_batch1/SQ00015167/analysis&quot;, &quot;groups&quot;: [ { &quot;Metadata&quot;: &quot;Metadata_Plate=SQ00015167,Metadata_Well=A01,Metadata_Site=1&quot; }, ... ] } Check the pipeline and data_file variables in the dcp_config.json file to ensure that they look right. They should be assigned relative paths e.g. projects/XXX/XXX/..., not absolute paths. This can happen if the PIPELINE_SET variable is not set correctly. Note that, as with illumination correction, this assumes that the pipeline has a _without_batchfile version of it in the same directory. cp_docker_commands.txt contains the commands to run the analysis pipeline on the current EC2 instance. In this example, the images were grouped by Plate, Well, and Site, and therefore each command processes a single site (=1 image). The first line of cp_docker_commands.txt for SQ00015167 is: docker run \\ --rm \\ --volume=/home/ubuntu/efs/2015_10_05_DrugRepurposing_AravindSubramanian_GolubLab_Broad/workspace/github/imaging-platform-pipelines/cellpainting_a549_20x_phenix_bin1:/pipeline_dir \\ --volume=/2016_04_01_a549_48hr_batch1/SQ00015167:/filelist_dir --volume=/home/ubuntu/efs/2015_10_05_DrugRepurposing_AravindSubramanian_GolubLab_Broad/workspace/load_data_csv/2016_04_01_a549_48hr_batch1/SQ00015167:/datafile_dir \\ --volume=/home/ubuntu/efs/2015_10_05_DrugRepurposing_AravindSubramanian_GolubLab_Broad/workspace/batchfiles/2016_04_01_a549_48hr_batch1/SQ00015167/analysis:/batchfile_dir \\ --volume=/home/ubuntu/efs/2015_10_05_DrugRepurposing_AravindSubramanian_GolubLab_Broad/workspace/analysis/2016_04_01_a549_48hr_batch1/SQ00015167/analysis:/output_dir \\ --volume=/home/ubuntu/efs/tmp/:/tmp_dir \\ --volume=/home/ubuntu/efs/2015_10_05_DrugRepurposing_AravindSubramanian_GolubLab_Broad/workspace/status/2016_04_01_a549_48hr_batch1/SQ00015167/analysis:/status_dir \\ --volume=/home/ubuntu/bucket/:/home/ubuntu/bucket/ \\ --log-driver=awslogs \\ --log-opt awslogs-group=SQ00015167 \\ --log-opt awslogs-stream=SQ00015167-A01-1 \\ cellprofiler/cellprofiler:2.3.1 \\ -p /batchfile_dir/Batch_data.h5 \\ -g Metadata_Plate=SQ00015167,Metadata_Well=A01,Metadata_Site=1 \\ --data-file=/datafile_dir/load_data_with_illum.csv \\ -o /output_dir \\ -t /tmp_dir \\ -d /status_dir/SQ00015167-A01-1.txt "],["run-jobs.html", "Chapter 5 Run Jobs 5.1 Illumination Correction 5.2 Quality Control 5.3 Analysis", " Chapter 5 Run Jobs 5.1 Illumination Correction 5.1.1 Single Node To compute illumination functions directly on the EC2 node, run the contents of cp_docker_commands.txt for each plate for PLATE_ID in $(cat ${PLATES}); do parallel -a ../../batchfiles/${BATCH_ID}/${PLATE_ID}/illum/cp_docker_commands.txt done If this is run on the current node, this is the resulting structure of analysis, containing the output of illum.cppipe, on EFS (one level below workspace). Files for only SQ00015167 are shown. └── 2016_04_01_a549_48hr_batch1 └── illum └── SQ00015167 ├── SQ00015167_IllumAGP.mat ├── SQ00015167_IllumDNA.mat ├── SQ00015167_IllumER.mat ├── SQ00015167_IllumMito.mat ├── SQ00015167_IllumRNA.mat └── SQ00015167.stderr Sync this folder to S3, maintaining the same structure. cd ~/efs/${PROJECT_NAME}/ aws s3 sync ${BATCH_ID}/illum/${PLATE_ID} s3://${BUCKET}/projects/${PROJECT_NAME}/${BATCH_ID}/illum/${PLATE_ID} 5.1.2 DCP Edit the config files illum_config.py and illum_config.json in cellpainting_scripts/dcp_config_files/ as needed. At the very least, replace the strings VAR_AWS_ACCOUNT_NUMBER,VAR_AWS_BUCKET,VAR_SUBNET_ID,VAR_GROUP_ID,VAR_KEYNAME, VAR_APP_NAME, VAR_DCP_VERSION with appropriate values. (See the DCP dockerhub to determine the best tag for your analysis) You do so using sed. The script below replaces the strings for both, analysis_* as well as illum_* config files. cd cellpainting_scripts/dcp_config_files/ for CONFIG_FILE in analysis_config.py analysis_config.json illum_config.py illum_config.json; do sed -i &quot;s/VAR_AWS_ACCOUNT_NUMBER/NNNNNNNNNNN/g&quot; ${CONFIG_FILE} sed -i &quot;s/VAR_AWS_BUCKET/name-of-s3-bucket/g&quot; ${CONFIG_FILE} sed -i &quot;s/VAR_SUBNET_ID/subnet-NNNNNNNN/g&quot; ${CONFIG_FILE} sed -i &quot;s/VAR_GROUP_ID/sg-NNNNNNNN/g&quot; ${CONFIG_FILE} sed -i &quot;s/VAR_KEYNAME/filename-of-key-file-without-extension/g&quot; ${CONFIG_FILE} sed -i &quot;s/VAR_APP_NAME/${PROJECT_NAME}/g&quot; ${CONFIG_FILE} sed -i &quot;s/VAR_DCP_VERSION/1.2.0_3.1.8/g&quot; ${CONFIG_FILE} done cd .. Copy to the DCP directory and setup the compute environment: cd ~/efs/${PROJECT_NAME}/workspace/software/Distributed-CellProfiler/ pyenv shell 2.7.12 cp ../cellpainting_scripts/dcp_config_files/illum_config.py config.py fab setup Submit jobs and start the cluster, then monitor: parallel \\ python run.py submitJob \\ ~/efs/${PROJECT_NAME}/workspace/batchfiles/${BATCH_ID}/{1}/illum/dcp_config.json :::: ${PLATES} python run.py \\ startCluster \\ ../cellpainting_scripts/dcp_config_files/illum_config.json # do this in a tmux session. python run.py monitor files/${PROJECT_NAME}_IllumSpotFleetRequestId.json Note that if your illumination correction takes longer than 12 hours to run (not uncommon, especially in CellProfiler 2.X), jobs may get started more than once. See the troubleshooting section of the DCP wiki. You should keep an eye on the output files directory ${PROJECT_NAME}/${BATCH_ID}/illum ; when all folders have the expected number of files, it's ok to purge the queue to kill all remaining machines. 5.2 Quality Control 5.2.1 Process QC Results into a Database for CPA cd ~/efs/${PROJECT_NAME}/workspace/software/ # If not cloned already, clone cytominer-database using http or ssh # e.g. http - git clone https://username@github.com/cytomining/cytominer-database # e.g. ssh - git clone git@github.com/cytomining/cytominer-database cd cytominer-database pyenv shell 3.5.1 pip install -e . cd ~/efs/${PROJECT_NAME}/workspace mkdir qc cytominer-database ingest \\ ~/bucket/projects/${PROJECT_NAME}/workspace/qc/${BATCH_ID}/results sqlite:///qc/${BATCH_ID}_QC.sqlite \\ -c software/cytominer-database/cytominer_database/config/config_default.ini \\ --no-munge rsync qc/${BATCH_ID}_QC.sqlite ~/bucket/projects/${PROJECT_NAME}/workspace/qc/${BATCH_ID}_QC.sqlite You can then download the database to your local machine; to update the S3 image paths to your local image paths you'll need to configure and execute the following SQL statement (DB Browser for SQLite, for example, allows you to do this easily in the GUI). You need only specify the parts of the paths that are different, not the whole path. UPDATE Image SET PathName_OrigBrightfield= REPLACE(PathName_OrigBrightfield, &#39;/home/ubuntu/bucket/projects/s3/path/to/files/&#39;, &#39;/local/path/to/files/&#39;) WHERE PathName_OrigBrightfield LIKE &#39;%/home/ubuntu/bucket/projects/%&#39;; UPDATE Image SET PathName_OrigAGP= REPLACE(PathName_OrigAGP, &#39;/home/ubuntu/bucket/projects/s3/path/to/files/&#39;, &#39;/local/path/to/files/&#39;) WHERE PathName_OrigAGP LIKE &#39;%/home/ubuntu/bucket/projects/%&#39;; UPDATE Image SET PathName_OrigDNA= REPLACE(PathName_OrigDNA, &#39;/home/ubuntu/bucket/projects/s3/path/to/files/&#39;, &#39;/local/path/to/files/&#39;) WHERE PathName_OrigDNA LIKE &#39;%/home/ubuntu/bucket/projects/%&#39;; UPDATE Image SET PathName_OrigER= REPLACE(PathName_OrigER, &#39;/home/ubuntu/bucket/projects/s3/path/to/files/&#39;, &#39;/local/path/to/files/&#39;) WHERE PathName_OrigER LIKE &#39;%/home/ubuntu/bucket/projects/%&#39;; UPDATE Image SET PathName_OrigMito= REPLACE(PathName_OrigMito, &#39;/home/ubuntu/bucket/projects/s3/path/to/files/&#39;, &#39;/local/path/to/files/&#39;) WHERE PathName_OrigMito LIKE &#39;%/home/ubuntu/bucket/projects/%&#39;; UPDATE Image SET PathName_OrigRNA= REPLACE(PathName_OrigRNA, &#39;/home/ubuntu/bucket/projects/s3/path/to/files/&#39;, &#39;/local/path/to/files/&#39;) WHERE PathName_OrigRNA LIKE &#39;%/home/ubuntu/bucket/projects/%&#39; Windows users must also then execute the following statements to change the direction of any slashes in the path. UPDATE Image SET PathName_OrigBrightfield= REPLACE(PathName_OrigBrightfield, &#39;/&#39;, &#39;\\&#39;) WHERE PathName_OrigBrightfield LIKE &#39;%/%&#39;; UPDATE Image SET PathName_OrigAGP= REPLACE(PathName_OrigAGP, &#39;/&#39;, &#39;\\&#39;) WHERE PathName_OrigAGP LIKE &#39;%/%&#39;; UPDATE Image SET PathName_OrigDNA= REPLACE(PathName_OrigDNA, &#39;/&#39;, &#39;\\&#39;) WHERE PathName_OrigDNA LIKE &#39;%/%&#39;; UPDATE Image SET PathName_OrigER= REPLACE(PathName_OrigER, &#39;/&#39;, &#39;\\&#39;) WHERE PathName_OrigER LIKE &#39;%/%&#39;; UPDATE Image SET PathName_OrigMito= REPLACE(PathName_OrigMito, &#39;/&#39;, &#39;\\&#39;) WHERE PathName_OrigMito LIKE &#39;%/%&#39;; UPDATE Image SET PathName_OrigRNA= REPLACE(PathName_OrigRNA, &#39;/&#39;, &#39;\\&#39;) WHERE PathName_OrigRNA LIKE &#39;%/%&#39; You can now configure your CPA properties file with the name of your new database and perform the QC. For more information on this process, see the Quality Control tutorial in the CellProfiler/tutorials repo. 5.3 Analysis 5.3.1 Single Node To run the analysis pipeline directly on the EC2 node, run the contents of cp_docker_commands.txt for each plate for PLATE_ID in $(cat ${PLATES}); do parallel -a ../../batchfiles/${BATCH_ID}/${PLATE_ID}/analysis/cp_docker_commands.txt done If this is run on the EC2 node, this is the resulting structure of analysis, containing the output of analysis.cppipe, on EFS (one level below workspace). Files for only SQ00015167 are shown. └── analysis    └── 2016_04_01_a549_48hr_batch1    └── SQ00015167       └── analysis       └── A01-1 ├── Cells.csv ├── Cytoplasm.csv ├── Experiment.csv ├── Image.csv ├── Nuclei.csv └── outlines ├── A01_s1--cell_outlines.png └── A01_s1--nuclei_outlines.png A01-1 is site 1 of well A01. In a 384-well plate, there will be 384*9 such folders. Note that the file Experiment.csv may get created one level above, i.e., under A01-1 (see https://github.com/CellProfiler/CellProfiler/issues/1110). Sync this folder to S3, maintaining the same structure. cd ~/efs/${PROJECT_NAME}/workspace/ aws s3 sync analysis/${BATCH_ID}/${PLATE_ID}/analysis s3://${BUCKET}/projects/${PROJECT_NAME}/workspace/analysis/${BATCH_ID}/${PLATE_ID}/analysis/ 5.3.2 DCP If not already done, edit the config files analysis_config.py and analysis_config.json in cellpainting_scripts/dcp_config_files/ as needed (see 5.1.2). Copy the analysis_config.py to the DCP directory and setup the compute environment. cd ~/efs/${PROJECT_NAME}/workspace/software/Distributed-CellProfiler/ pyenv shell 2.7.12 cp ../cellpainting_scripts/dcp_config_files/analysis_config.py config.py fab setup Submit jobs and start the cluster: parallel \\ python run.py submitJob \\ ~/efs/${PROJECT_NAME}/workspace/batchfiles/${BATCH_ID}/{1}/analysis/dcp_config.json :::: ${PLATES} python run.py \\ startCluster \\ ../cellpainting_scripts/dcp_config_files/analysis_config.json Start the monitor. Do this in a tmux session. Note: Unless you run the monitor, the fleet will never be killed! python run.py monitor files/${PROJECT_NAME}_AnalysisSpotFleetRequestId.json "],["create-profiles.html", "Chapter 6 Create Profiles 6.1 Confirm Environment Configuration 6.2 Create Database Backend 6.3 Annotate 6.4 Normalize 6.5 Select Variables 6.6 Execute Feature Selection 6.7 Aggregate Replicates 6.8 Audit 6.9 Convert to Other Formats 6.10 Upload Data", " Chapter 6 Create Profiles 6.1 Confirm Environment Configuration If you are starting from here, make sure the following steps have been completed on your ec2 instance and/or session before proceeding Configure Environment for Full Profiling Pipeline Download software Create list of plates 6.2 Create Database Backend Run creation of sqlite backend as well as aggregation of measurements into per-well profiles. This process can be very slow since the files are read from s3fs/EFS. We recommend first downloading the CSVs files locally on an EBS volume attached to the ec2 instance you are running on, and then ingesting. To do so, first recreate the analysis output folder structure on the EBS volume: mkdir -p ~/ebs_tmp/${PROJECT_NAME}/workspace/software cd ~/ebs_tmp/${PROJECT_NAME}/workspace/software if [ -d cytominer_scripts ]; then rm -rf cytominer_scripts; fi git clone https://github.com/broadinstitute/cytominer_scripts.git cd cytominer_scripts The command below first calls cytominer-database ingest to create the SQLite backend, and then aggregate.R to create per-well profiles. Once complete, all files are uploaded to S3 and the local cache are deleted. collate.R ingests the database and then calls aggregate.R. pyenv shell 3.5.1 mkdir -p ../../log/${BATCH_ID}/ parallel \\ --max-procs ${MAXPROCS} \\ --eta \\ --joblog ../../log/${BATCH_ID}/collate.log \\ --results ../../log/${BATCH_ID}/collate \\ --files \\ --keep-order \\ ./collate.R \\ --batch_id ${BATCH_ID} \\ --plate {1} \\ --config ingest_config.ini \\ --tmpdir ~/ebs_tmp \\ --download \\ --remote_base_dir s3://${BUCKET}/projects/${PROJECT_NAME}/workspace :::: ${PLATES} collate.R does not recreate the SQLite backend if it already exists in the local cache. Add --overwrite_backend_cache flag to recreate. For pipelines that use FlagImage to skip the measurements modules if the image failed QC, the failed images will have Image.csv files with fewer columns that the rest (because columns corresponding to aggregated measurements will be absent). The ingest command will show a warning related to sqlite: expected X columns but found Y - filling the rest with NULL. This is expected behavior. There is a known issue where if the alphabetically-first CSV failed QC in a pipeline where &quot;Skip image if flagged&quot; is turned on, the databases will not be created. We are working to fix this, but in the meantime we recommend either not skipping processing of your flagged images (and removing them from your data downstream) or deleting the alphabetically-first CSVs until you come to one where the pipeline ran completely. This is the resulting structure of backend on S3 (one level below workspace) for SQ00015167: └── backend    └── 2016_04_01_a549_48hr_batch1 └── SQ00015167 ├── SQ00015167.csv └── SQ00015167.sqlite SQ00015167.sqlite is the per cell data and SQ00015167.csv is the aggregated per-well data. 6.2.1 Copy Files from S3 to EFS Copy these files from S3 to EFS to continue with the rest of the processing cd ~/efs/${PROJECT_NAME}/workspace/software/cytominer_scripts aws s3 sync --exclude &quot;*.sqlite&quot; \\ s3://${BUCKET}/projects/${PROJECT_NAME}/workspace/backend/${BATCH_ID}/ \\ ~/efs/${PROJECT_NAME}/workspace/backend/${BATCH_ID}/ rsync -arzv ~/ebs_tmp/${PROJECT_NAME}/workspace/log/ ../../log 6.2.2 Quick Check Rows Do a quick check to view how many rows are present in each of the aggregated per-well data. parallel \\ --no-run-if-empty \\ --keep-order \\ wc -l ../../backend/${BATCH_ID}/{1}/{1}.csv :::: ${PLATES} 6.2.3 Something Amiss? Check the error logs. step=collate parallel \\ --no-run-if-empty \\ --keep-order \\ head ../../log/${BATCH_ID}/${step}/1/{1}/stderr :::: ${PLATES} 6.3 Annotate First, get metadata for the plates. This should be created beforehand and be made available in S3. We use annotate.R for this procedure. aws s3 sync \\ s3://${BUCKET}/projects/${PROJECT_NAME}/workspace/metadata/${BATCH_ID}/ \\ ~/efs/${PROJECT_NAME}/workspace/metadata/${BATCH_ID}/ This is the resulting structure of the metadata folder on EFS (one level below workspace): Be super careful about matching names! └── metadata    └── 2016_04_01_a549_48hr_batch1    ├── barcode_platemap.csv    └── platemap    └── C-7161-01-LM6-006.txt 2016_04_01_a549_48hr_batch1 is the batch name – the plates (and all related data) are arranged under batches, as seen below. barcode_platemap.csv is structured as shown below. Assay_Plate_Barcode and Plate_Map_Name are currently the only mandatory columns (they are used to join the metadata of the plate map with each assay plate). Each unique entry in the Plate_Map_Name should have a corresponding tab-separated file .txt file under platemap (e.g. C-7161-01-LM6-006.txt). Assay_Plate_Barcode,Plate_Map_Name SQ00015167,C-7161-01-LM6-006 The tab-separated files are plate maps and are structured like this: (This is the typical format followed by Broad Chemical Biology Platform) plate_map_name well_position broad_sample mg_per_ml mmoles_per_liter solvent C-7161-01-LM6-006 A07 BRD-K18895904-001-16-1 3.12432000000000016 9.99999999999999999 DMSO C-7161-01-LM6-006 A08 BRD-K18895904-001-16-1 1.04143999999919895 3.33333333333076923 DMSO C-7161-01-LM6-006 A09 BRD-K18895904-001-16-1 0.347146666668001866 1.11111111111538462 DMSO plate_map_name should be identical to the name of the file (without extension). plate_map_name and well_position are currently the only mandatory columns. If your experiment has two or more cell lines, but the same plate map, create one plate map file for each cell line, e.g. C-7161-01-LM6-006_A549.txt, rename the plate_map_name to the name of the file without extension (e.g. C-7161-01-LM6-006_A549), add a column cell_id, and populate it with the name of the cell line (e.g. A549). Make sure the plate maps for all lines are reflected in the barcode_platemap.csv file. Next, append the metadata to the aggregated per-well data. 6.3.1 Example 1: Simple Metadata cd ~/efs/${PROJECT_NAME}/workspace/software/cytominer_scripts parallel \\ --no-run-if-empty \\ --eta \\ --joblog ../../log/${BATCH_ID}/annotate.log \\ --results ../../log/${BATCH_ID}/annotate \\ --files \\ --keep-order \\ ./annotate.R \\ --batch_id ${BATCH_ID} \\ --plate_id {1} :::: ${PLATES} 6.3.2 Example 2: Complex Metadata This is an example coming from Broad chemical experiment. Use the -j flag to optionally append columns from another source (EXTERNAL_METADATA below). EXTERNAL_METADATA should be a CSV file. The columns that are in common with the aggregated CSV file will be used to join. See the annotate source for details. Use the -c flag to optionally specify the cell type. cd ~/efs/${PROJECT_NAME}/workspace/software/cytominer_scripts EXTERNAL_METADATA=../../metadata/${BATCH_ID}/cell_painting_dataset_cmap_annotations_moa.csv parallel \\ --no-run-if-empty \\ --eta \\ --joblog ../../log/${BATCH_ID}/annotate.log \\ --results ../../log/${BATCH_ID}/annotate \\ --files \\ --keep-order \\ ./annotate.R \\ --batch_id ${BATCH_ID} \\ --plate_id {1} \\ --format_broad_cmap \\ --cell_id A549 \\ --external_metadata ${EXTERNAL_METADATA} \\ --perturbation_mode chemical :::: ${PLATES} 6.3.3 Expected Folder Structure This is the resulting structure of backend on EFS (one level below workspace) for SQ00015167: └── backend    └── 2016_04_01_a549_48hr_batch1 └── SQ00015167 ├── SQ00015167_augmented.csv └── SQ00015167.csv SQ00015167_augmented.csv is the aggregated per-well data, annotated with metadata. 6.3.4 Quick Check Rows Do a quick check to view how many rows are present in each of the annotated per-well data. parallel \\ --no-run-if-empty \\ --keep-order \\ wc -l ../../backend/${BATCH_ID}/{1}/{1}_augmented.csv :::: ${PLATES} 6.3.5 Something Amiss? Check the error logs. step=annotate parallel \\ --no-run-if-empty \\ --keep-order \\ head ../../log/${BATCH_ID}/${step}/1/{1}/stderr :::: ${PLATES} 6.4 Normalize Use all wells on the plate to normalize each feature. By default, this performs robust z-scoring per feature. The default input is the annotated per-well data. The column picked for normalization (e.g. &quot;Metadata_Well&quot;) needs to be present in the CSV; if it was added in the augmentation step (e.g. &quot;cell_id&quot;) it will now have the prefix &quot;Metadata&quot; prepended to it) We use normalize.R for this procedure. parallel \\ --no-run-if-empty \\ --eta \\ --joblog ../../log/${BATCH_ID}/normalize.log \\ --results ../../log/${BATCH_ID}/normalize \\ --files \\ --keep-order \\ ./normalize.R \\ --batch_id ${BATCH_ID} \\ --plate_id {1} \\ --subset \\&quot;Metadata_Well != \\&#39;\\&#39;\\&#39;dummy\\&#39;\\&#39;\\&#39;\\&quot; :::: ${PLATES} Don't escape quotes if not using parallel i.e. use --subset &quot;Metadata_Well != '''dummy'''&quot; if not using within parallel. To use a different reference distribution to compute the median and m.a.d. for z-scoring, change the filter specified using the --subset flag. 6.4.1 Expected Folder Structure This is the resulting structure of backend on EFS (one level below workspace) for SQ00015167: └── backend    └── 2016_04_01_a549_48hr_batch1 └── SQ00015167 ├── SQ00015167_augmented.csv ├── SQ00015167.csv └── SQ00015167_normalized.csv SQ00015167_normalized.csv is the robust z-scored (normalized) per-well data. 6.4.2 Quick Check Rows Do a quick check to view how many rows are present in each of the normalized per-well data. parallel \\ --no-run-if-empty \\ --keep-order \\ wc -l ../../backend/${BATCH_ID}/{1}/{1}_normalized.csv :::: ${PLATES} 6.4.3 Something Amiss? Check the error logs. step=normalize parallel \\ --no-run-if-empty \\ --keep-order \\ head ../../log/${BATCH_ID}/${step}/1/{1}/stderr :::: ${PLATES} 6.5 Select Variables Create samples to do variable selection. Sample some wells from each replicate plate (if your experiment contains multiple copies of identical plates). Below, this is done by sample 2 entire replicate plates per platemap. Use -n (--replicates) to specify number of replicate plates to be used to create the sample (if you don't have identical replicate plates in your experiment, you can set this to 1). 6.5.1 Sample Data Samples are created for both, normalized and unnormalized data, because the variable selection techniques may require both. We use sample.R for this procedure. mkdir -p ../../parameters/${BATCH_ID}/sample/ # sample normalized data ./sample.R \\ --batch_id ${BATCH_ID} \\ --pattern &quot;_normalized.csv$&quot; \\ --replicates 2 \\ --output ../../parameters/${BATCH_ID}/sample/${BATCH_ID}_normalized_sample.feather # sample unnormalized data ./sample.R \\ --batch_id ${BATCH_ID} \\ --pattern &quot;_augmented.csv$&quot; \\ --replicates 2 \\ --output ../../parameters/${BATCH_ID}/sample/${BATCH_ID}_augmented_sample.feather 6.5.2 Preselect We use preselect.R for these procedures. 6.5.2.1 Replicate Correlation Make a list of variables to be preserved after replicate_correlation variable selection is performed. If you don't have replicate plates in this experiment, skip this step. To evaluate features for their replicability, use all the normalized profiles of treatments in the experiment (selected below using Metadata_broad_sample_type == 'trt'). ./preselect.R \\ --batch_id ${BATCH_ID} \\ --input ../../parameters/${BATCH_ID}/sample/${BATCH_ID}_normalized_sample.feather \\ --operations replicate_correlation \\ --subset &quot;Metadata_broad_sample_type == &#39;&#39;&#39;trt&#39;&#39;&#39;&quot; \\ --replicates 2 6.5.2.2 Correlation Threshold Make a list of variables to be preserved after correlation_threshold variable selection is performed. ./preselect.R \\ --batch_id ${BATCH_ID} \\ --input ../../parameters/${BATCH_ID}/sample/${BATCH_ID}_normalized_sample.feather \\ --operations correlation_threshold 6.5.2.3 Variance Threshold Make a list of variables to be preserved after variance_threshold variable selection is performed. To evaluate the variance of features, use only the control wells the experiment (selected below using Metadata_broad_sample_type == 'control'). ./preselect.R \\ --batch_id ${BATCH_ID} \\ --input ../../parameters/${BATCH_ID}/sample/${BATCH_ID}_augmented_sample.feather \\ --operations variance_threshold \\ --subset &quot;Metadata_broad_sample_type == &#39;&#39;&#39;control&#39;&#39;&#39;&quot; 6.5.2.4 Noise Threshold Some variables have previously identified as being noisy or non-informative. Create a list of variables that excludes these variables. # manually remove some features echo &quot;variable&quot; &gt; ../../parameters/${BATCH_ID}/variable_selection/manual.txt head -1 \\ ../../backend/${BATCH_ID}/${SAMPLE_PLATE_ID}/${SAMPLE_PLATE_ID}.csv \\ |tr &quot;,&quot; &quot;\\n&quot;|grep -v Meta|grep -E -v &#39;Granularity_14|Granularity_15|Granularity_16|Manders|RWC|Costes&#39; &gt;&gt; \\ ../../parameters/${BATCH_ID}/variable_selection/manual.txt 6.5.3 Alternate: Load previous preselect configuration You may have already performed these steps for a different batch of data, and want to simply copy the parameters to this batch. Here's how you'd copy these files. mkdir -p ../../parameters/${BATCH_ID}/variable_selection/ REFERENCE_BATCH_ID=2018_02_23_LKCP_DBG aws s3 sync \\ s3://${BUCKET}/projects/${PROJECT_NAME}/workspace/parameters/${REFERENCE_BATCH_ID}/ \\ ~/efs/${PROJECT_NAME}/workspace/parameters/${REFERENCE_BATCH_ID}/ rsync -arzv ../../parameters/${REFERENCE_BATCH_ID}/variable_selection/ ../../parameters/${BATCH_ID}/variable_selection/ 6.6 Execute Feature Selection We use select.R for this procedures. The previous steps only create a list of variable to be preserved for each variable selection method. To actually apply variable selection, we compute the intersection of all these variable lists, then preserve only those columns of the normalized per-well data. In the filters argument, exclude the variable selection methods that you do not want to use. parallel \\ --no-run-if-empty \\ --eta \\ --joblog ../../log/${BATCH_ID}/select.log \\ --results ../../log/${BATCH_ID}/select \\ --files \\ --keep-order \\ ./select.R \\ --batch_id ${BATCH_ID} \\ --plate_id {1} \\ --filters variance_threshold,replicate_correlation,correlation_threshold,manual :::: ${PLATES} 6.6.1 Expected Folder Structure This is the resulting structure of backend on EFS (one level below workspace) for SQ00015167: └── backend    └── 2016_04_01_a549_48hr_batch1 └── SQ00015167 ├── SQ00015167_augmented.csv ├── SQ00015167.csv ├── SQ00015167_normalized.csv └── SQ00015167_normalized_variable_selected.csv SQ00015167_normalized_variable_selected.csv is the variable-selected version of the normalized per-well data. 6.6.2 Quick Check Rows Do a quick check to view how many rows are present in each of the normalized per-well data. parallel \\ --no-run-if-empty \\ --keep-order \\ wc -l ../../backend/${BATCH_ID}/{1}/{1}_normalized_variable_selected.csv :::: ${PLATES} 6.6.3 Something Amiss? Check the error logs. step=select parallel \\ --no-run-if-empty \\ --keep-order \\ head ../../log/${BATCH_ID}/${step}/1/{1}/stderr :::: ${PLATES} 6.7 Aggregate Replicates Combine replicate plates of each plate map by averaging (default is mean). mkdir -p ../../collated/${BATCH_ID}/ PLATE_MAPS=../../scratch/${BATCH_ID}/plate_maps.txt csvcut -c Plate_Map_Name \\ ../../metadata/${BATCH_ID}/barcode_platemap.csv | \\ tail -n +2|sort|uniq &gt; \\ ${PLATE_MAPS} parallel \\ --no-run-if-empty \\ --eta \\ --joblog ../../log/${BATCH_ID}/collapse.log \\ --results ../../log/${BATCH_ID}/collapse \\ --keep-order \\ ./collapse.R \\ --batch_id ${BATCH_ID} \\ --plate_map_name {1} \\ --suffix _normalized_variable_selected.csv \\ --output ../../collated/${BATCH_ID}/{1}_collapsed.csv :::: ${PLATE_MAPS} 6.7.1 Expected Folder Structure This is the resulting structure of collated on EFS (one level below workspace) for 2016_04_01_a549_48hr_batch1: └── collated    └── 2016_04_01_a549_48hr_batch1 └── C-7161-01-LM6-006_collapsed.csv C-7161-01-LM6-006_collapsed.csv is the replicate averaged data for plate map C-7161-01-LM6-006. 6.7.2 Quick Check Rows Do a quick check to view how many rows are present in the replicate averaged data of each plate map. parallel \\ --no-run-if-empty \\ --keep-order \\ wc -l ../../collated/${BATCH_ID}/{1}_collapsed.csv :::: ${PLATE_MAPS} 6.7.3 Combine Averaged Profiles Combine all averaged profiles in the batch into a single file. mkdir -p ../../log/${BATCH_ID}/collate The columns should be identical across all CSVs. Let's check for this. First create a list of columns names, per CSV file: parallel \\ &quot;csvcut -n ../../collated/${BATCH_ID}/{1}_collapsed.csv &gt; ../../log/${BATCH_ID}/collate/{1}_colnames.txt&quot; :::: ${PLATE_MAPS} Next, verify that they are identical diff -q --from-file `parallel echo ../../log/${BATCH_ID}/collate/{1}_colnames.txt :::: ${PLATE_MAPS}` csvstack stacks CSVs even if they don't have identical columns, so proceed with the next step only if you have verified that the columns are identical. csvstack \\ `parallel echo ../../collated/${BATCH_ID}/{1}_collapsed.csv :::: ${PLATE_MAPS}` &gt; \\ ../../collated/${BATCH_ID}/${BATCH_ID}_collapsed.csv 6.8 Audit Audit each plate map for replicate reproducibility. We use audit.R for these procedures. This will only work if your experiment contains multiple copies of identical plates; if it does not, you may skip this step. mkdir -p ../../audit/${BATCH_ID}/ 6.8.1 Treatment Audit Audit only treated wells parallel \\ --no-run-if-empty \\ --eta \\ --joblog ../../log/${BATCH_ID}/audit.log \\ --results ../../log/${BATCH_ID}/audit \\ --files \\ --keep-order \\ ./audit.R \\ --batch_id ${BATCH_ID} \\ --plate_map_name {1} \\ --suffix _normalized_variable_selected.csv \\ --subset \\&quot;Metadata_broad_sample_type == \\&#39;\\&#39;\\&#39;trt\\&#39;\\&#39;\\&#39;\\&quot; \\ --output ../../audit/${BATCH_ID}/{1}_audit.csv \\ --output_detailed ../../audit/${BATCH_ID}/{1}_audit_detailed.csv \\ --group_by Metadata_Plate_Map_Name,Metadata_moa,Metadata_pert_id,Metadata_broad_sample,Metadata_mmoles_per_liter,Metadata_Well :::: ${PLATE_MAPS} 6.8.2 Control Audit Audit only control wells, i.e., how well do control wells in the same position correlate? parallel \\ --no-run-if-empty \\ --eta \\ --joblog ../../log/${BATCH_ID}/audit_control.log \\ --results ../../log/${BATCH_ID}/audit_control \\ --files \\ --keep-order \\ ./audit.R \\ --batch_id ${BATCH_ID} \\ --plate_map_name {1} \\ --suffix _normalized_variable_selected.csv \\ --subset \\&quot;Metadata_broad_sample_type == \\&#39;\\&#39;\\&#39;control\\&#39;\\&#39;\\&#39;\\&quot; \\ --output ../../audit/${BATCH_ID}/{1}_audit_control.csv \\ --output_detailed ../../audit/${BATCH_ID}/{1}_audit_control_detailed.csv \\ --group_by Metadata_Well :::: ${PLATE_MAPS} 6.9 Convert to Other Formats We use csv2gct.R for these procedures. These GCT files can be examined in many programs, though we routinely use Morpheus. 6.9.1 Convert per-plate CSV files to GCT parallel \\ --no-run-if-empty \\ --eta \\ --joblog ../../log/${BATCH_ID}/csv2gct_backend.log \\ --results ../../log/${BATCH_ID}/csv2gct_backend \\ --files \\ --keep-order \\ ./csv2gct.R \\ ../../backend/${BATCH_ID}/{1}/{1}_{2}.csv \\ --output ../../backend/${BATCH_ID}/{1}/{1}_{2}.gct :::: ${PLATES} ::: augmented normalized normalized_variable_selected 6.9.2 Convert per-plate map CSV files to GCT parallel \\ --no-run-if-empty \\ --eta \\ --joblog ../../log/${BATCH_ID}/csv2gct_collapsed.log \\ --results ../../log/${BATCH_ID}/csv2gct_collapsed \\ --files \\ --keep-order \\ ./csv2gct.R \\ ../../collated/${BATCH_ID}/{1}_collapsed.csv \\ --output ../../collated/${BATCH_ID}/{1}_collapsed.gct :::: ${PLATE_MAPS} 6.9.3 Convert the replicate-collapsed CSV file to gct ./csv2gct.R \\ ../../collated/${BATCH_ID}/${BATCH_ID}_collapsed.csv \\ --output ../../collated/${BATCH_ID}/${BATCH_ID}_collapsed.gct 6.10 Upload Data 6.10.1 Sync to S3 parallel \\ aws s3 sync \\ ../../{1}/${BATCH_ID}/ \\ s3://${BUCKET}/projects/${PROJECT_NAME}/workspace/{1}/${BATCH_ID}/ ::: audit backend batchfiles collated load_data_csv log metadata parameters scratch 6.10.2 Sync Down from S3 onto a Machine Specify location for syncing LOCAL_FS=/cmap/imaging Set variables on your local machine matching those set on your EC2 instance PROJECT_NAME=2015_10_05_DrugRepurposing_AravindSubramanian_GolubLab_Broad BATCH_ID=2016_04_01_a549_48hr_batch1 6.10.3 Sync the files echo audit backend batchfiles collated load_data_csv log metadata parameters scratch | \\ tr &quot; &quot; &quot;\\n&quot; | xargs -I % \\ aws s3 sync \\ --exclude &quot;*.sqlite&quot; \\ s3://${BUCKET}/projects/${PROJECT_NAME}/workspace/%/${BATCH_ID}/ \\ ${LOCAL_FS}/${PROJECT_NAME}/workspace/%/${BATCH_ID}/ "],["appendix-a.html", "A Appendix A A.1 Project folder structure guidance A.2 Directory structure", " A Appendix A A.1 Project folder structure guidance The guidances notes provide an overview of the folder structure used in this handbook All projects live in an S3 bucket The directory structure is always &lt;bucket-name&gt;/projects/&lt;project_name&gt; In the &lt;project_name&gt; folder there are sub directories The first subdirectory is workspace Other subdirectories are batches of data The batches of data are labeled by date and include images and illum folders In the images folder there exist different plates storing raw image data The illum folder is identical to the images folder in terms of structure illum is an output of the first stage of cell profiler pipeline that stores a function to adjust the plates in images workspace also has subdirectories analysis - includes subfolders mirroring the Batch nesting Within each batch folder, the CellProfiler results are stored in plate_id Within each plate folder there is an analysis folder Inside this analysis folder, each well has its own folder (e.g. A01-1) A and 01 refer to the row and column of the plate, 1 refer to sites per well If the grouping was done by well instead of by site, this would be A01, without the suffix of -1 Note that this analysis folder is customizable There are typically 384 (# of wells) x 9 (# of sites per well) subfolders 384 well plate 9 different pictures Within the site folder (e.g. A01-1) there are five csv files Cells.csv Each row are measurements of one cell Cytoplasm.csv Another object similar to Cells.csv Nuclei.csv Another object similar to Cells.csv These three object files can be concatenated by column Objects.csv Experiment.csv Stores metadata for the CellProfiler run, including the CellProfiler pipeline itself Image.csv backend - also includes batch nesting batch nesting plate nesting - stores summaries of each plate (all .csv files also have .gct formats (for input into Morpheus) &lt;plate_id&gt;.sqlite - inner join of all objects in a well, and then stacked (so all data for each well in a single plate) &lt;plate_id&gt;.csv - per well means for each well on the plate &lt;plate_id&gt;.augmented.csv - same as .csv except it includes the metadata &lt;plate_id&gt;._normalized.csv - some z scored version of augmented &lt;plate_id&gt;._normalized_variable_selected.csv - across all the plates in the batch Three feature selection steps Variance threshold Correlation threshold (decorrelate feature set) Replicate correlation filter (&gt;0.6) parameters - same structure as backend but with metadata results (e.g. the features selected in variable selection) software This is where the project's github repository lives. The scripts in the handbook assume that this be named as the same name as the Project folder. To rename it, pay careful attention to paths when executing the commands in the handbook. A.2 Directory structure ├── 2016_04_01_a549_48hr_batch1 │ ├── illum │ │ └── SQ00015167 │ │ ├── SQ00015167_IllumAGP.mat │ │ ├── SQ00015167_IllumDNA.mat │ │ ├── SQ00015167_IllumER.mat │ │ ├── SQ00015167_IllumMito.mat │ │ ├── SQ00015167_IllumRNA.mat │ │ └── SQ00015167.stderr │ └── images │ └── SQ00015167__2016-04-21T03_34_00-Measurement1 │ ├── Assaylayout │ ├── FFC_Profile │ └── Images │ ├── r01c01f01p01-ch1sk1fk1fl1.tiff │ ├── r01c01f01p01-ch2sk1fk1fl1.tiff │ ├── r01c01f01p01-ch3sk1fk1fl1.tiff │ ├── r01c01f01p01-ch4sk1fk1fl1.tiff │ └── r01c01f01p01-ch5sk1fk1fl1.tiff └── workspace ├── audit │    └── 2016_04_01_a549_48hr_batch1 ├── analysis │    └── 2016_04_01_a549_48hr_batch1 │    └── SQ00015167 │       └── analysis │       └── A01-1 │ ├── Cells.csv │ ├── Cytoplasm.csv │ ├── Experiment.csv │ ├── Image.csv │ ├── Nuclei.csv │ └── outlines │ └── SQ00015167 │ ├── A01_s1--cell_outlines.png │ └── A01_s1--nuclei_outlines.png ├── backend │   └── 2016_04_01_a549_48hr_batch1 │   └── SQ00015167 │ ├── SQ00015167.csv │ └── SQ00015167.sqlite ├── batchfiles │ └── 2016_04_01_a549_48hr_batch1 │ └── SQ00015167 │ ├── analysis │ │ ├── Batch_data.h5 │ │ ├── dcp_config.json │ │ ├── cp_docker_commands.txt │ │ └── cpgroups.csv │ └── illum │ ├── Batch_data.h5 │ ├── dcp_config.json │ ├── cp_docker_commands.txt │ └── cpgroups.csv ├── github │   └── imaging-platform-pipelines ├── images │   └── 2016_04_01_a549_48hr_batch1 -&gt; /home/ubuntu/bucket/projects/2015_10_05_DrugRepurposing_AravindSubramanian_GolubLab_Broad/2016_04_01_a549_48hr_batch1/images/ ├── load_data_csv │   └── 2016_04_01_a549_48hr_batch1 │   └── SQ00015167 │   ├── load_data.csv │   └── load_data_with_illum.csv ├── log │   ├── create_batch_files_analysis │   ├── create_batch_files_illum │   ├── create_csv_from_xml │   └── collate   ├── metadata │   └── 2016_04_01_a549_48hr_batch1 │   ├── barcode_platemap.csv │   └── platemap │   └── C-7161-01-LM6-006.txt ├── pipelines -&gt; github/imaging-platform-pipelines ├── status └── software ├── cellpainting_scripts ├── cytominer_scripts └── pe2loaddata "]]
