[["index.html", "Image-based Profiling Handbook Preface", " Image-based Profiling Handbook Beth Cimini, Tim Becker, Shantanu Singh, Gregory Way, Hamdah Abbasi 2021-06-25 Preface "],["introduction.html", "Chapter 1 Introduction 1.1 Collect your software 1.2 Collect your pipelines 1.3 Determine how to get your image lists to CellProfiler 1.4 Execute your CellProfiler pipelines 1.5 Analysis 1.6 Aggregate your data 1.7 Create and manipulate per-well profiles.", " Chapter 1 Introduction This handbook describes the process of running a Cell Painting experiment. While the code here will describe doing so in the context of running Distributed-CellProfiler on AWS on images generated by a PerkinElmer microscope, then collating the data with cytominer-database and analyzing it with pycytominer, the basic procedure for running a Cell Painting experiment is the same no matter the microscope or the processing platform. Briefly, the steps for any and every platform are: 1.1 Collect your software For the specific use case here, this involves Distributed-CellProfiler pe2loaddata cytominer-database pycytominer along with their dependencies. Almost certainly, you will need a locally-to-your-images installed version of GUI CellProfiler that matches the version you want to run on your cluster - locally might mean on a local machine or a VM. 1.2 Collect your pipelines You will minimally require these pipelines illum (illumination correction) analysis (segmentation and feature extraction) But you may also want pipelines for Z projection QC assay development 1.3 Determine how to get your image lists to CellProfiler CellProfiler needs to understand image sets, aka for each field-of-view that was captured how many channels were there, what would you like to name the channels were there, and what are the file names corresponding to each channel. If you are using a Phenix Opera or Operetta, you can use the pe2loaddata program to generate a CSV that contains a list of image sets in an automated fashion, which can be passed to CellProfiler in via the LoadData module. Otherwise, you have a couple of different options: You can create a similar CSV using a script that you write yourself that handles the files from your microscope and makes a similar CSV. Minimally, you need a FileName and PathName column for each channel (ie FileName_OrigDNA), and Metadata columns for each piece of metadata CellProfiler needs (ie Metadata_Plate, Metadata_Well, and Metadata_Site) You can use a local copy of the files CellProfiler will be running, configure the 4 input modules of CellProfiler to create your image sets, then export CSVs using CellProfiler's &quot;Export Image Set Listing&quot; option, and feed those into the pipelines to be run on your cluster. Alter all of your pipelines to, rather than the LoadData CSV module, use the 4 input modules of CellProfiler and add, configure, and run each pipeline with the CreateBatchFiles module and use batch files in your cluster environment. These options are ordered in terms of most-scripting-proficiency-needed to least-scripting-proficiency-needed as well as least-CellProfiler-proficiency-needed to most-CellProfiler-proficiency-needed. 1.4 Execute your CellProfiler pipelines 1.4.1 (Optional) Z projection If your images were taken with multiple planes, you will need to Z-project them. All subsequent steps should be run on the projected images. 1.4.2 (Optional) QC You may want to run a quality control pipeline to determine your imaging plate quality. You can choose to run this locally or on your cluster compute environment. You will need to evaluate the results of this pipeline somehow, in CellProfiler-Analyst, KNIME, SpotFire, etc. You may run illumination correction and assay development steps in the meantime, but should hold analysis steps until the results are evaluated. 1.4.3 Illumination correction You need to run a pipeline that is grouped by plate and creates an illumination correction function. Since it is grouped by plate, you don't need very many CPUs to run this, but it will take 6-24 hours depending on settings and image size. Assay development and analysis require this step to complete. 1.4.4 (Optional) Assay Development If desired, you can run a pipeline that executes on one image per well and carries out your segmentation but not measurement steps and makes (an) image(s) that you can use to evaluate the quality of the segmentation (either individually or by stitching them together first). This is not required but allows you to ensure that your segmentation parameters look reasonable across the variety of phenotypes present in your data. If being run, the final step should be held until this step can be evaluated. 1.5 Analysis This pipeline segments the cells and measures the whole images and cells, and creates output CSVs (or can dump to a MySQL host if configured). This is typically run on each image site in parallel and thus can be sped up by using a large number of CPUs. 1.6 Aggregate your data Since the analysis is run in parallel, unless using a MySQL host you will have a number of sets of CSVs that need to be turned into a single file per plate. This is currently done with the cytominer-database program. 1.7 Create and manipulate per-well profiles. The final step is to create per-well profiles, annotate them with metadata, and do steps such as plate normalization and feature selection. These are accomplished via a &quot;profiling recipe&quot; using pycytominer. "],["configure-environment-for-full-profiling-pipeline.html", "Chapter 2 Configure Environment for Full Profiling Pipeline 2.1 Launch an AWS Virtual Machine for making CSVs and running Distributed-CellProfiler 2.2 Create a tmux session 2.3 Define Environment Variables 2.4 Create Directories 2.5 Download Software", " Chapter 2 Configure Environment for Full Profiling Pipeline This workflow assumes you have already set up an AWS account with an S3 bucket and EFS, and created a VM per the instructions in the link below. 2.1 Launch an AWS Virtual Machine for making CSVs and running Distributed-CellProfiler Launch an EC2 node using AMI cytomining/images/hvm-ssd/cytominer-bionic-trusty-18.04-amd64-server-*, created using cytominer-vm. You will need to create an AMI for your own infrastructure because the provisioning includes mounting S3 and EFS, which is account specific. We recommend using an m4.xlarge instance, with an 8Gb EBS volume. Note: Proper configuration is essential to mount the S3 bucket. The following configuration provides an example, named imaging-platform (modifications will be necessary). Launch an ec2 instance on AWS AMI: cytomining/images/hvm-ssd/cytominer-ubuntu-trusty-18.04-amd64-server-1529668435 Instance Type: m4.xlarge Network: vpc-35149752 Subnet: Default (imaging platform terraform) IAM role: s3-imaging-platform-role No Tags Select Existing Security Group: SSH_HTTP Review and Launch ssh -i &lt;USER&gt;.pem ubuntu@&lt;Public DNS IPv4&gt; After starting the instance, ensure that the S3 bucket is mounted on ~/bucket. If not, run sudo mount -a. Log in to the EC2 instance. Enter your AWS credentials aws configure The infrastructure is configured with one S3 bucket. Mount this S3 bucket (if it is not automatically mounted) sudo mount -a Check that the bucket was was mounted. This path should exist: ls ~/bucket/projects 2.2 Create a tmux session You will want to retain environment variables once defined, and for processes to run when you are not connected, so you should create a tmux session to work in tmux new -s sessionname You can detach from this session at any time by typing Ctl+b, then d. To reattach to an existing session, type tmux a -t sessionname You can list existing sessions with tmux list-sessions and kill any poorly-behaving session with tmux kill-session -t sessionname 2.3 Define Environment Variables These variables will be used throughout the project to tag instances, logs etc so that you know which machines are working on what, which files to operate on, where your logs are, etc. PROJECT_NAME=2015_10_05_DrugRepurposing_AravindSubramanian_GolubLab_Broad BATCH_ID=2016_04_01_a549_48hr_batch1 BUCKET=imaging-platform MAXPROCS=3 # m4.xlarge has 4 cores; this should be # of cores on your instance - 1 2.4 Create Directories mkdir -p ~/efs/${PROJECT_NAME}/workspace/ cd ~/efs/${PROJECT_NAME}/workspace/ mkdir -p log/${BATCH_ID} 2.5 Download Software cd ~/efs/${PROJECT_NAME}/workspace/ mkdir software cd software git clone https://github.com/broadinstitute/pe2loaddata.git git clone https://github.com/CellProfiler/Distributed-CellProfiler.git cd .. If these repos have already been cloned, git pull to make sure they are up to date. This is the resulting structure of software on EFS (one level below workspace): └── software ├── Distributed-CellProfiler └── pe2loaddata "],["setup-images.html", "Chapter 3 Setup Images 3.1 Upload Images 3.2 Prepare Images 3.3 Create List of Plates 3.4 Create LoadData CSVs 3.5 Upload image location files to S3", " Chapter 3 Setup Images 3.1 Upload Images Your image files should be uploaded to AWS from your local compute environment via a tool like Cyberduck or the AWS CLI (aws s3 sync /local/path s3://BUCKET/PROJECT_NAME/BATCH_ID/images) (see also Appendix A.2 for more information on folder structures). Some important tips BEFORE uploading (these are much more difficult to fix once uploaded): Ensure your image sets are complete i.e. all image sets should have the same number of channels and z-planes, and that this is true across the entire batch of plates you are processing. Avoid folder names with spaces Plate names should not have leading 0's (ie 123 not 000123) VERY IMPORTANT: If using pe2loaddata (described later) to generate your image CSVs, please ensure the folder name contains the plate name given when imaging on the Phenix microscope (can be checked in the Index.idx.xml) 3.2 Prepare Images (if using pe2loaddata to create image sets) Create soft link to the image folder. Note that the relevant S3 bucket has been mounted at /home/ubuntu/bucket/. The folder structure for images differs between S3 and EFS. This can be potentially confusing. However note that the step below simply creates a soft link to the images in S3; no files are copied. Further, when pe2loaddata is run, the --sub-string-out and --sub-string-in flags ensure the resulting LoadData CSV files end up having the paths to the images as they exist on S3. Thus the step below (of creating a softlink) only serves the purpose of making the images folder have a similar structure as the others (e.g. load_data_csv, metadata, analysis). If you’re Z-projecting images and the unprojected images are in a folder with a different name (such as /unprojected_images/), you should create the soft link to that folder: ln -s ~/bucket/projects/${PROJECT_NAME}/${BATCH_ID}/unprojected_images/ ${BATCH_ID} cd ~/efs/${PROJECT_NAME}/workspace/ mkdir images #Run this only if this is the first batch for this project cd images ln -s ~/bucket/projects/${PROJECT_NAME}/${BATCH_ID}/images/ ${BATCH_ID} cd .. This is the resulting structure of the image folder on EFS (one level below workspace): └── images    └── 2016_04_01_a549_48hr_batch1 -&gt; /home/ubuntu/bucket/projects/2015_10_05_DrugRepurposing_AravindSubramanian_GolubLab_Broad/2016_04_01_a549_48hr_batch1/images/ This is the structure of the image folder on S3 (one level above workspace, under the folder 2016_04_01_a549_48hr_batch1.) Here, only one plate (SQ00015167__2016-04-21T03_34_00-Measurement1) is show but there are often many more. └── images    └── 2016_04_01_a549_48hr_batch1 └── SQ00015167__2016-04-21T03_34_00-Measurement1    ├── Assaylayout    ├── FFC_Profile    └── Images ├── r01c01f01p01-ch1sk1fk1fl1.tiff ├── r01c01f01p01-ch2sk1fk1fl1.tiff ├── r01c01f01p01-ch3sk1fk1fl1.tiff ├── r01c01f01p01-ch4sk1fk1fl1.tiff └── r01c01f01p01-ch5sk1fk1fl1.tiff SQ00015167__2016-04-21T03_34_00-Measurement1 is the typical nomenclature followed by Broad Chemical Biology Platform for plate names. Measurement1 indicates the first attempt to image the plate. Measurement2 indicates second attempt and so on. 3.3 Create List of Plates (if using pe2loaddata to create image sets) Create a text file with one plate id per line. The plate IDs, if using pe2loaddata, must match the plate IDs given when operating the Phenix. Otherwise, they should match CellProfiler's understanding of the Plate grouping variable, whether that is explicitly stated in a loaddata CSV OR produced from the Metadata module if the CSVs and/or batch files are created using CellProfiler's input modules. For downstream purposes, i.e. cytominer, you may choose to use only so much of the plate name as you need to keep the plates unique (e.g. SQ00015167 instead of SQ00015167__2016-04-21T03_34_00-Measurement1 to keep the names compact. mkdir -p ~/efs/${PROJECT_NAME}/workspace/scratch/${BATCH_ID}/ PLATES=$(readlink -f ~/efs/${PROJECT_NAME}/workspace/scratch/${BATCH_ID}/plates_to_process.txt) FULL_PLATES=$(readlink -f ~/efs/${PROJECT_NAME}/workspace/scratch/${BATCH_ID}/full_plates_to_process.txt) ls ~/efs/${PROJECT_NAME}/workspace/images/${BATCH_ID}/ | cut -d &#39;_&#39; -f 1 &gt;&gt; $PLATES ls ~/efs/${PROJECT_NAME}/workspace/images/${BATCH_ID}/ &gt;&gt; $FULL_PLATES Check that your plate names are correct by nano $PLATES and nano $FULL_PLATES. If your plate names contain underscores, you may need to fix the simplified plate names. Both should have the same number of rows as the number of plates in your batch. SAMPLE_PLATE_ID=PLATE_NAME_1 This can be any single plate name, again using the portion of the name that is before the double underscore __. SAMPLE_FULL_PLATE_NAME=FULL_PLATE_NAME_1 This can be any single plate name, this time using the full name. 3.4 Create LoadData CSVs (if using pe2loaddata to create image sets) The script below works only for Phenix microscopes – it reads a standard XML file (Index.idx.xml) and writes a LoadData CSV file. For other microscopes, you will have to roll your own (see the overview chapter for more information). The script below requires config.yml, which specifies the mapping between channel names in Index.idx.xml and the channel names in the CellProfiler pipelines metadata to extract from Index.idx.xml Here's a truncated sample config.yml (here is an example of the full file) channels: HOECHST 33342: OrigDNA Alexa 568: OrigAGP Alexa 647: OrigMito Alexa 488: OrigER 488 long: OrigRNA Brightfieldlow: OrigBrightfield metadata: Row: Row Col: Col FieldID: FieldID PlaneID: PlaneID ChannelID: ChannelID ChannelName: ChannelName ImageResolutionX: ImageResolutionX [...] Often, the values of the keys for channels are different in Index.idx.xml, so for example, above, we have Brightfieldlow: OrigBrightfield but the keys for channels could be different in Index.idx.xml: $ tail -n 500 ~/efs/${PROJECT_NAME}/workspace/images/${BATCH_ID}/${SAMPLE_FULL_PLATE_NAME}/Images/Index.idx.xml|grep ChannelName|sort -u &lt;ChannelName&gt;488 long&lt;/ChannelName&gt; &lt;ChannelName&gt;Alexa 488&lt;/ChannelName&gt; &lt;ChannelName&gt;Alexa 568&lt;/ChannelName&gt; &lt;ChannelName&gt;Alexa 647&lt;/ChannelName&gt; &lt;ChannelName&gt;Brightfield CP&lt;/ChannelName&gt; &lt;ChannelName&gt;HOECHST 33342&lt;/ChannelName&gt; Copy the text into a text file somewhere on your computer so you can refer to it. Now navigate to your pe2loaddata repository and ensure that it is up to date cd ~/efs/${PROJECT_NAME}/workspace/software/pe2loaddata git pull pyenv shell 3.8.10 pip3 install -e . Adjust any discrepancies between the list of channels from your index file and the config by editing config.yml: HOECHST 33342: OrigDNA Alexa 568: OrigAGP Alexa 647: OrigMito Alexa 488: OrigER 488 long: OrigRNA Brightfield CP: OrigBrightfield Ensure that all the metadata fields defined in config.yml are present in the Index.idx.xml. Ensure that the channel names are the same in config.yml and Index.idx.xml Ensure that the LoadData csv files don't already exist; if they do, delete them. The max-procs option is set as 1 because pe2loaddata accesses the image files on s3fs, which doesn't handle multiple requests well. If your images require Z projection, make sure that sub-string-in is set to the folder that you soft-linked to in the previous step. pyenv shell 3.8.10 parallel \\ --link \\ --max-procs 1 \\ --eta \\ --joblog ../../log/${BATCH_ID}/create_csv_from_xml.log \\ --results ../../log/${BATCH_ID}/create_csv_from_xml \\ --files \\ --keep-order \\ pe2loaddata config.yml \\ ~/efs/${PROJECT_NAME}/workspace/load_data_csv/${BATCH_ID}/{1}/load_data.csv \\ --index-directory ~/efs/${PROJECT_NAME}/workspace/images/${BATCH_ID}/{2}/Images \\ --illum \\ --illum-directory /home/ubuntu/bucket/projects/${PROJECT_NAME}/${BATCH_ID}/illum/{1} \\ --plate-id {1} \\ --illum-output ~/efs/${PROJECT_NAME}/workspace/load_data_csv/${BATCH_ID}/{1}/load_data_with_illum.csv \\ --sub-string-out efs/${PROJECT_NAME}/workspace/images/${BATCH_ID} \\ --sub-string-in bucket/projects/${PROJECT_NAME}/${BATCH_ID}/images :::: ${PLATES} ${FULL_PLATES} This is the resulting structure of load_data_csv on EFS (one level below workspace). Files for only SQ00015167 are shown. └── load_data_csv    └── 2016_04_01_a549_48hr_batch1    └── SQ00015167    ├── load_data.csv    └── load_data_with_illum.csv load_data.csv will be used by illum.cppipe and, optionally, qc.cppipe. load_data_with_illum.csv will be used by analysis.cppipe and, optionally, assaydev.cppipe. When creating load_data_with_illum.csv, the script assumes a specific location for the folder containing the illumination correction files. If your files must be Z projected, your load_data.csv will be correct for that step. Once that step is executed, edit BOTH of your CSVs to ensure that The Orig image paths are updated to the location of the projected files rather than than the unprojected files Only the last-numbered plane from each site are kept These steps can be done manually in e.g. in Excel but are easier to script for large numbers of plates. You should then upload your edited CSVs to S3. 3.5 Upload image location files to S3 If using pe2loaddata, run the command below Copy the load data files to S3: aws s3 sync \\ ~/efs/${PROJECT_NAME}/workspace/load_data_csv/${BATCH_ID}/ \\ s3://${BUCKET}/projects/${PROJECT_NAME}/workspace/load_data_csv/${BATCH_ID}/ If using your own home-created load data CSVs, load them to the same location - s3://${BUCKET}/projects/${PROJECT_NAME}/workspace/load_data_csv/${BATCH_ID} - we strongly recommend the structure of making one subfolder with CSVs in it per plate and then using the names load_data.csv for CSVs without illum files and load_data_with_illum.csv for those with the illum files. If using batch files, we recommend uploading them to s3://${BUCKET}/projects/${PROJECT_NAME}/workspace/batchfiles/${BATCH_ID}, giving each batch file a unique name to reflect which step it is for - since the process of creating batchfiles can be onerous, subsequent steps assume you will make one batchfile per batch rather than per plate, but you may adjust this if you like. "],["run-each-cellprofiler-step.html", "Chapter 4 Run each CellProfiler step 4.1 Upload your pipelines to S3 4.2 Configure Distributed-CellProfiler's run_batch_general script 4.3 Configure Distributed-CellProfiler's fleet file 4.4 Change required parameters in Distributed-CellProfiler's config file 4.5 Run each CellProfiler step 4.6 (Optional) Do any post-CellProfiler steps", " Chapter 4 Run each CellProfiler step The steps below are outlined specifically for running Distributed CellProfiler on AWS. If you are not doing so, the steps will still be more-or-less the same Make sure CellProfiler pipelines are accessible Make sure CellProfiler knows where the input images are, either via a CSV or a batch file Run each CellProfiler pipeline, in sequence, with appropriate input folder, output folder, and grouping 4.1 Upload your pipelines to S3 In your project's workspace directory, create a batch specific folder and upload your pipelines there. If there are previous batches from the same project or a similar one, you may find it easiest to copy the files directly. Once uploaded and/or copied, the file structure should look like the below. └── pipelines └── 2016_04_01_a549_48hr_batch1 ├── illum_without_batchfile.cppipe └── analysis_without_batchfile.cppipe 4.2 Configure Distributed-CellProfiler's run_batch_general script Note that run_batch_general is not required; the Distributed CellProfiler handbook lays out a number of different ways of creating jobs. However, we find it the most efficient way to run numerous pipelines on the same data. If you do not wish to use it, you can adjust steps 3 and 4 in the &quot;Run each CellProfiler step&quot; to &quot;Create a job file&quot; and &quot;Execute python3 run.py submitJob jobFileName.json&quot; run_batch_general.py can be configured once at the beginning of the run of a batch of data, and then can be run for each step simply by uncommenting the name of the step to run. The following variables in the project specific stuff section of the script should be configured: topdirname and batchsuffix should match your PROJECT_NAME and BATCH_ID, respectively appname is typically the same as topdirname, but if that name is long and cumbersome you can create an abbreviated version here (ie 2015_10_05_DrugRepurposing rather than 2015_10_05_DrugRepurposing_AravindSubramanian_GolubLab_Broad). This will be used in your config.py file rows, columns, and sites should reflect the imaging conditions used platelist should contain a list of plates, comma separated, ie ['SQ00015167','SQ00015168'] If you are using pipeline files with the LoadData module and CSVs, you should make sure that the pipeline names reflect your pipeline names (or adjust if not). Otherwise, you should make sure that the batch file names reflect your batch file names. If following the recommended structures and procedures, none of the not project specific section of the script should need to be adapted, but if you are making changes you may need to. 4.3 Configure Distributed-CellProfiler's fleet file If running in a fresh clone of Distributed-CellProfiler, you will need to configure a single fleet file, which will be used in all subsequent steps. Refer to the manual for instructions. 4.4 Change required parameters in Distributed-CellProfiler's config file If running in a fresh clone of Distributed-CellProfiler, you will need to set the AWS_REGION, SSH_KEY_NAME, AWS_BUCKET, and SQS_DEAD_LETTER_QUEUE settings to appropriate settings for your account. Refer to the manual for instructions. 4.5 Run each CellProfiler step You may have as many as 5 or as few as 2 CellProfiler steps (optional) Z projection (optional) QC - see also section 4.6 illumination correction (optional) assay development - see also section 4.6 analysis. For each step, the steps you will run will be identical: Configure the config.py file Execute python3 run.py setup Uncomment the correct step name in your run_batch_general.py file (and ensure all other steps are commented out) Execute python3 run_batch_general.py Execute python3 run.py startCluster files/yourFleetFileName.json, where you have set the name of the fleet file previously created or located Execute python3 run.py monitor files/APP_NAMESpotFleetRequestId.json, where APP_NAME matches the APP_NAME variable set in step 1. Information on all of these steps is available in the Distributed-CellProfiler wiki. You need only absolutely change the variables stated above and below for Distributed-CellProfiler to function, but other variables may be useful, such as using a non-default profile, adjusting whether or not you would like to pre-download files and/or use plugins and/or restart only parts of a batch of data. In general, as long as you are running inside a tmux session and it isn't killed, the monitor should destroy any and all infrastructure created on AWS as part of the running Distributed-CellProfiler, but it is the user's responsibility to check that this has completed appropriately; failure to do so may lead to spot fleets generating charges after all useful work has completed. 4.5.1 (Optional) Z projection Your APP_NAME variable should be set to the appname set in run_batch_general.py plus _Zproj, ie 2015_10_05_DrugRepurposing_Zproj Your number of CLUSTER_MACHINES should be medium-large, ie a hundred or few hundred. Your SQS_MESSAGE_VISIBILITY should be short, such as 5*60 (5 minutes) 4.5.2 (Optional) QC Your APP_NAME variable should be set to the appname set in run_batch_general.py plus _QC, ie 2015_10_05_DrugRepurposing_QC Your number of CLUSTER_MACHINES should be medium-large, ie a hundred or few hundred. Your SQS_MESSAGE_VISIBILITY should be short, such as 5*60 (5 minutes) 4.5.3 Illumination Correction Your APP_NAME variable should be set to the appname set in run_batch_general.py plus _Illum, ie 2015_10_05_DrugRepurposing_Illum Your number of CLUSTER_MACHINES should be set to the number of plates you have divided by 4 then rounded up, ie 6 for 22 plates Your SQS_MESSAGE_VISIBILITY should be 12 hours 720*60 4.5.4 (Optional) Assay Dev Your APP_NAME variable should be set to the appname set in run_batch_general.py plus _AssayDev, ie 2015_10_05_DrugRepurposing_AssayDev Your number of CLUSTER_MACHINES should be medium-large, ie a hundred or few hundred. Your SQS_MESSAGE_VISIBILITY should be short, such as 5*60 (5 minutes) 4.5.5 Analysis Your APP_NAME variable should be set to the appname set in run_batch_general.py plus _Analysis, ie 2015_10_05_DrugRepurposing_Analysis Your number of CLUSTER_MACHINES should be as many as possible per your account limits, ideally at least a few hundred. Your SQS_MESSAGE_VISIBILITY should be 10-20 minutes for images with a binning of 2, longer (30-120 minutes) for unbinned images and/or CellProfiler 2 or 3 runs. This value is the most potentially variable -once you've run a single analysis workflow, you can adjust this value accordingly based on your log files. 4.6 (Optional) Do any post-CellProfiler steps The optional QC and assay development steps have post-CellProfiler components. These pipelines need only be run if the user plans to do these post-CellProfiler steps. QC may require visual inspection of images, creation of a machine learning classifier to detect poor quality images, and/or running scripts to evaluate CV. How best to evaluate quality is left to the user. The assay development step creates images to be visually evaluated, either individually or after stitching for easier evaluation. If your segmentation is not ideal, you may need to update your assay dev and analysis pipelines by manually tuning the segmentation steps on a local set of representative data until they perform better on your images, then making sure to update the segmentation in the assay dev and analysis pipelines on your cluster accordingly. After running the final analysis pipelines, proceed to the next step of this guide. "],["create-profiles.html", "Chapter 5 Create Profiles 5.1 Confirm Environment Configuration 5.2 Add a large EBS volume to your machine 5.3 Create Database Backend", " Chapter 5 Create Profiles 5.1 Confirm Environment Configuration You CAN, if you choose, make your backends with the same machine that you have used to make CSVs and run DCP for CellProfiler. However, we typically do not, since backend creation can take a long time so it is desirable to use 2 machines - a small inexpensive machine with only a few CPUs for all the steps before backends and a larger machine with at least as many CPUs as (the number of plates in your batch +1) for backend creation. Both machines can be turned off when not in use and when off will generate only minimal charges. To make a new backend machine, follow the identical instructions as the in the section below, only with a larger Instance Type (such as an m4.10xlarge). Launch an AWS Virtual Machine for making CSVs and running Distributed-CellProfiler Since backend creation also typically requires a large amount of hard disk space, which you pay for whether or not the machine is on, we recommend attaching a large EBS volume to your backend creation machine only when needed, then detaching and terminating it when not in use. 5.2 Add a large EBS volume to your machine Follow the AWS instructions for creating and attaching the EBS volume. Two critical factors to note- the volume must be created in the same subnet as your backend creation machine, and should be approximately 2X the size as the analysis files in your batch - this is most easily figured by navigating to that location in the S3 web console and selecting &quot;Actions -&gt; Calculate total size&quot;. Once the volume is created and attached, ensure the machine is started and SSH connect to it. Create a temp directory which is required when creating the database backed using cytominer-database (discussed later). mkdir ~/ebs_tmp Get the name of the disk and attach it. # check the name of the disk lsblk #&gt; NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT #&gt; xvda 202:0 0 8G 0 disk #&gt; └─xvda1 202:1 0 8G 0 part / #&gt; xvdba 202:80 0 100G 0 disk # check if it has a file system sudo file -s /dev/xvdba # ...likely not, in which case you get: #&gt; /dev/xvdf: data # if no file system, then create it sudo mkfs -t ext4 /dev/xvdba # mount it sudo mount /dev/xvdba /home/ubuntu/ebs_tmp/ # change perm sudo chmod 777 ~/ebs_tmp/ If you are starting from here, make sure the following steps have been completed on your ec2 instance and/or session before proceeding Configure Environment for Full Profiling Pipeline Create list of plates 5.3 Create Database Backend Run creation of sqlite backend as well as aggregation of measurements into per-well profiles. This process can be very slow since the files are read from s3fs/EFS. We recommend first downloading the CSVs files locally on an EBS volume attached to the ec2 instance you are running on, and then ingesting. To do so, first recreate the analysis output folder structure on the EBS volume: mkdir -p ~/ebs_tmp/${PROJECT_NAME}/workspace/software cd ~/ebs_tmp/${PROJECT_NAME}/workspace/software if [ -d pycytominer ]; then rm -rf pycytominer; fi git clone https://github.com/cytomining/pycytominer.git cd pycytominer git checkout jump python3 -m pip install -e . The command below first calls cytominer-database ingest to create the SQLite backend, and then pycytominer's aggregate_profiles to create per-well profiles. Once complete, all files are uploaded to S3 and the local cache are deleted. collate.py ingests and indexes the database. pyenv shell 3.8.10 mkdir -p ../../log/${BATCH_ID}/ parallel \\ --max-procs ${MAXPROCS} \\ --ungroup \\ --eta \\ --joblog ../../log/${BATCH_ID}/collate.log \\ --results ../../log/${BATCH_ID}/collate \\ --files \\ --keep-order \\ python3 pycytominer/cyto_utils/collate.py ${BATCH_ID} pycytominer/cyto_utils/ingest_config.ini {1} \\ --temp ~/ebs_tmp \\ --remote=s3://${BUCKET}/projects/${PROJECT_NAME}/workspace :::: ${PLATES} collate.py does not recreate the SQLite backend if it already exists in the local cache. Add --overwrite flag to recreate. For pipelines that use FlagImage to skip the measurements modules if the image failed QC, the failed images will have Image.csv files with fewer columns that the rest (because columns corresponding to aggregated measurements will be absent). The ingest command will show a warning related to sqlite: expected X columns but found Y - filling the rest with NULL. This is expected behavior. There is a known issue where if the alphabetically-first CSV failed QC in a pipeline where &quot;Skip image if flagged&quot; is turned on, the databases will not be created. We are working to fix this, but in the meantime we recommend either not skipping processing of your flagged images (and removing them from your data downstream) or deleting the alphabetically-first CSVs until you come to one where the pipeline ran completely. This is the resulting structure of backend on S3 (one level below workspace) for SQ00015167: └── backend    └── 2016_04_01_a549_48hr_batch1 └── SQ00015167 ├── SQ00015167.csv └── SQ00015167.sqlite At this point, the user needs to use the profiling template to use pycytominer to annotate the profiles with metadata, normalize them, and feature select them. Detailed instructions for these steps will be added as soon as possible. "],["appendix-a.html", "A Appendix A A.1 Project folder structure guidance A.2 Directory structure", " A Appendix A A.1 Project folder structure guidance The guidances notes provide an overview of the folder structure used in this handbook All projects live in an S3 bucket The directory structure is always &lt;bucket-name&gt;/projects/&lt;project_name&gt; In the &lt;project_name&gt; folder there are sub directories The first subdirectory is workspace Other subdirectories are batches of data The batches of data are labeled by date and include images and illum folders In the images folder there exist different plates storing raw image data The illum folder is identical to the images folder in terms of structure illum is an output of the first stage of cell profiler pipeline that stores a function to adjust the plates in images workspace also has subdirectories analysis - includes subfolders mirroring the Batch nesting Within each batch folder, the CellProfiler results are stored in plate_id Within each plate folder there is an analysis folder Inside this analysis folder, each well has its own folder (e.g. A01-1) A and 01 refer to the row and column of the plate, 1 refer to sites per well If the grouping was done by well instead of by site, this would be A01, without the suffix of -1 Note that this analysis folder is customizable There are typically 384 (# of wells) x 9 (# of sites per well) subfolders 384 well plate 9 different pictures Within the site folder (e.g. A01-1) there are five csv files Cells.csv Each row are measurements of one cell Cytoplasm.csv Another object similar to Cells.csv Nuclei.csv Another object similar to Cells.csv These three object files can be concatenated by column Objects.csv Experiment.csv Stores metadata for the CellProfiler run, including the CellProfiler pipeline itself Image.csv backend - also includes batch nesting batch nesting plate nesting - stores summaries of each plate (all .csv files also have .gct formats (for input into Morpheus) &lt;plate_id&gt;.sqlite - inner join of all objects in a well, and then stacked (so all data for each well in a single plate) &lt;plate_id&gt;.csv - per well means for each well on the plate &lt;plate_id&gt;.augmented.csv - same as .csv except it includes the metadata &lt;plate_id&gt;._normalized.csv - some z scored version of augmented &lt;plate_id&gt;._normalized_variable_selected.csv - across all the plates in the batch Three feature selection steps Variance threshold Correlation threshold (decorrelate feature set) Replicate correlation filter (&gt;0.6) parameters - same structure as backend but with metadata results (e.g. the features selected in variable selection) software This is where the project's github repository lives. The scripts in the handbook assume that this be named as the same name as the Project folder. To rename it, pay careful attention to paths when executing the commands in the handbook. A.2 Directory structure ├── 2016_04_01_a549_48hr_batch1 │ ├── illum │ │ └── SQ00015167 │ │ ├── SQ00015167_IllumAGP.mat │ │ ├── SQ00015167_IllumDNA.mat │ │ ├── SQ00015167_IllumER.mat │ │ ├── SQ00015167_IllumMito.mat │ │ ├── SQ00015167_IllumRNA.mat │ │ └── SQ00015167.stderr │ └── images │ └── SQ00015167__2016-04-21T03_34_00-Measurement1 │ ├── Assaylayout │ ├── FFC_Profile │ └── Images │ ├── r01c01f01p01-ch1sk1fk1fl1.tiff │ ├── r01c01f01p01-ch2sk1fk1fl1.tiff │ ├── r01c01f01p01-ch3sk1fk1fl1.tiff │ ├── r01c01f01p01-ch4sk1fk1fl1.tiff │ └── r01c01f01p01-ch5sk1fk1fl1.tiff └── workspace ├── audit │    └── 2016_04_01_a549_48hr_batch1 ├── analysis │    └── 2016_04_01_a549_48hr_batch1 │    └── SQ00015167 │       └── analysis │       └── A01-1 │ ├── Cells.csv │ ├── Cytoplasm.csv │ ├── Experiment.csv │ ├── Image.csv │ ├── Nuclei.csv │ └── outlines │ └── SQ00015167 │ ├── A01_s1--cell_outlines.png │ └── A01_s1--nuclei_outlines.png ├── backend │   └── 2016_04_01_a549_48hr_batch1 │   └── SQ00015167 │ ├── SQ00015167.csv │ └── SQ00015167.sqlite ├── images │   └── 2016_04_01_a549_48hr_batch1 -&gt; /home/ubuntu/bucket/projects/2015_10_05_DrugRepurposing_AravindSubramanian_GolubLab_Broad/2016_04_01_a549_48hr_batch1/images/ ├── load_data_csv │   └── 2016_04_01_a549_48hr_batch1 │   └── SQ00015167 │   ├── load_data.csv │   └── load_data_with_illum.csv ├── log │   ├── create_csv_from_xml │   └── collate   ├── metadata │   └── 2016_04_01_a549_48hr_batch1 │   ├── barcode_platemap.csv │   └── platemap │   └── C-7161-01-LM6-006.txt ├── pipelines ├── status └── software ├── Distributed-CellProfiler └── pe2loaddata "]]
