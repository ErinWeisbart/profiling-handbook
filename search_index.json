[
["index.html", "Image-based Profiling Handbook Preface", " Image-based Profiling Handbook Tim Becker, Beth Cimini, Shantanu Singh 2018-06-20 Preface "],
["configure-environment.html", "Chapter 1 Configure environment 1.1 Set up a virtual machine 1.2 Define variables 1.3 Create directories", " Chapter 1 Configure environment 1.1 Set up a virtual machine This example assumes that AWS infrastructure has been set up using https://github.com/broadinstitute/aws-infrastructure-cellpainting Launch an EC2 node using AMI cytomining/images/hvm-ssd/cytominer-ubuntu-trusty-14.04-amd64-server-*, created using https://github.com/broadinstitute/imaging-vms/blob/master/cytominer/cytominer_ami.json. You will need to create an AMI for your own infrastructure because the provisioning includes mounting S3 and EFS, which is account specific. See module &quot;ec2_login&quot; in the Terraform configuration for how to configure the security groups and IAM roles for this instance. The simplest approach is to launch another node identical to ec2_login, which is set up in this infrastructure. We recommend using an m4.xlarge instance, with a 110Gb EBS volume. After starting the instance, ensure that the S3 bucket is mount on ~/bucket. If not, do sudo mount -a Troubleshooting: Note that given this configuration in this AMI, EFS can only be mounted from us-east-1a or 1b. This can be changed by appropriately editing the EFS configuration via Terraform. Log in to the EC2 instance Check available space on the instance du -h Ensure that the Available column is at least 30Gb x p, where p is the number of plates you will process in parallel when creating the database backend. We recommend p to be one less than the number of cores (p = 3 for m4.xlarge, so 60Gb should be available) Enter your AWS credentials aws configure The infrastructure is configure with one S3 bucket. Mount this S3 bucket (if it is not automatically mounted) sudo mount -a Check that the bucket was was mounted. This path should exist: ls ~/bucket/projects 1.2 Define variables PROJECT_NAME=2015_10_05_DrugRepurposing_AravindSubramanian_GolubLab_Broad BATCH_ID=2016_04_01_a549_48hr_batch1 BUCKET=imaging-platform MAXPROCS=3 # m4.xlarge has 4 cores; keep 1 free 1.3 Create directories *Troublshooting tip:* See note above about EFS - that it can only be mounted from us-east-1a or 1b. mkdir -p ~/efs/${PROJECT_NAME}/workspace/ cd ~/efs/${PROJECT_NAME}/workspace/ mkdir -p log/${BATCH_ID} Create a temp directory which is required when creating the database backed using cytominer-database (discussed later). This is also useful if you decide to run CellProfiler directly on this node – running the Cell Painting analysis spipeline results in large temporary files. mkdir ~/ebs_tmp *Troublshooting tip:* If at this point you realize that the ec2 instance doesn’t have enough space (which you can check using du -h), create and attach an EBS volume, and then mount it. # check the name of the disk lsblk #&gt; NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT #&gt; xvda 202:0 0 8G 0 disk #&gt; └─xvda1 202:1 0 8G 0 part / #&gt; xvdf 202:80 0 100G 0 disk # check if it has a file system sudo file -s /dev/xvdf # ...likely not, in which case you get: #&gt; /dev/xvdf: data # if no file system, then create it sudo mkfs -t ext4 /dev/xvdf # mount it sudo mount /dev/xvdf /home/ubuntu/ebs_tmp/ # change perm sudo chmod 777 ~/ebs_tmp/ "],
["configure-tools-to-process-images.html", "Chapter 2 Configure tools to process images 2.1 Download software 2.2 Setup Distributed CellProfiler", " Chapter 2 Configure tools to process images 2.1 Download software You may need to authenticate your GitHub account to be able to clone these: cd ~/efs/${PROJECT_NAME}/workspace/ mkdir software cd software git clone git@github.com:broadinstitute/cellpainting_scripts.git git clone git@github.com:broadinstitute/pe2loaddata.git git clone git@github.com:broadinstitute/cytominer_scripts.git cd .. To authenticate your GitHub account, follow instructions to setup SSH keys here https://help.github.com/articles/connecting-to-github-with-ssh/. You can also clone these repos by following the structure below and inputting your OAuth token as necessary. git clone https://username@github.com/broadinstitute/cytominer_scripts.git If these repos have already been cloned, git pull to make sure they are up to date. This is the resulting structure of software on EFS (one level below workspace): └── software ├── cellpainting_scripts ├── cytominer_scripts └── pe2loaddata 2.2 Setup Distributed CellProfiler cd ~/efs/${PROJECT_NAME}/workspace/software/Distributed-CellProfiler/ pyenv shell 2.7.12 pip install -r files/requirements.txt "],
["setup-pipelines-and-images.html", "Chapter 3 Setup pipelines and images 3.1 Get CellProfiler pipelines 3.2 Specify pipeline set 3.3 Prepare images 3.4 Create list of plates 3.5 Create LoadData CSVs", " Chapter 3 Setup pipelines and images 3.1 Get CellProfiler pipelines Cell Painting pipelines are stored in a GitHub repo. If you are using a new pipeline, be sure to add it to the repo first. Follow instructions on https://github.com/broadinstitute/imaging-platform-pipelines for adding new pipelines. cd ~/efs/${PROJECT_NAME}/workspace/ mkdir github cd github/ git clone git@github.com:broadinstitute/imaging-platform-pipelines.git cd .. ln -s github/imaging-platform-pipelines pipelines This is the resulting structure of github and pipelines on EFS (one level below workspace): ├── github │    └── imaging-platform-pipelines └── pipelines -&gt; github/imaging-platform-pipelines 3.2 Specify pipeline set PIPELINE_SET=cellpainting_a549_20x_with_bf_phenix_bin1 Ensure that, both, analysis.cppipe as well as illum.cppipe are present for this set. As well, each pipeline should have a _without_batchfile version of it in the same directory. It’s easy to create such a version of the pipeline - simply copy it and set enabled=False for the CreateBatchFiles module (like here). 3.3 Prepare images Create soft link to the image folder, which should be uploaded on S3. Note that the relevant S3 bucket has been mounted at /home/ubuntu/bucket/. *Troublshooting tip:* The folder structure for images differs between S3 and EFS. This can be potentially confusing. However note that the step below simply creates a soft link to the images in S3; no files are copied. Further, when pe2loaddata is run (later in the process, via create_csv_from_xml.sh) it resolves the soft link, so the the resulting LoadData CSV files end up having the paths to the images as they exist on S3. Thus the step below (of creating a softlink) only serves the purpose of making the images folder have a similar structure as the others (e.g. load_data_csv, metadata, analysis). cd ~/efs/${PROJECT_NAME}/workspace/ mkdir images cd images ln -s ~/bucket/projects/${PROJECT_NAME}/${BATCH_ID}/images/ ${BATCH_ID} cd .. This is the resulting structure of the image folder on EFS (one level below workspace): └── images    └── 2016_04_01_a549_48hr_batch1 -&gt; /home/ubuntu/bucket/projects/2015_10_05_DrugRepurposing_AravindSubramanian_GolubLab_Broad/2016_04_01_a549_48hr_batch1/images/ This is the structure of the image folder on S3 (one level above workspace, under the folder 2016_04_01_a549_48hr_batch1.) Here, only one plate (SQ00015167__2016-04-21T03_34_00-Measurement1) is show but there are often many more. └── images    └── 2016_04_01_a549_48hr_batch1 └── SQ00015167__2016-04-21T03_34_00-Measurement1    ├── Assaylayout    ├── FFC_Profile    └── Images ├── r01c01f01p01-ch1sk1fk1fl1.tiff ├── r01c01f01p01-ch2sk1fk1fl1.tiff ├── r01c01f01p01-ch3sk1fk1fl1.tiff ├── r01c01f01p01-ch4sk1fk1fl1.tiff └── r01c01f01p01-ch5sk1fk1fl1.tiff SQ00015167__2016-04-21T03_34_00-Measurement1 is the typical nomenclature followed by Broad Chemical Biology Platform for plate names. Measurement1 indicates the first attempt to image the plate. Measurement2 indicates second attempt and so on. Ensure that there’s only one folder corresponding to a plate before running create_csv_from_xml.sh below (it gracefully exits if not). 3.4 Create list of plates Create a text file with one plate id per line. mkdir -p scratch/${BATCH_ID}/ PLATES=$(readlink -f ~/efs/${PROJECT_NAME}/workspace/scratch/${BATCH_ID}/plates_to_process.txt) echo &quot;SQ00015130 SQ00015168 SQ00015167 SQ00015166 SQ00015165&quot;|tr &quot; &quot; &quot;\\n&quot; &gt; ${PLATES} 3.5 Create LoadData CSVs The script below works only for Phenix microscopes – it reads a standard XML file (Index.idx.xml) and writes a LoadData csv file. For other microscopes, you will have to roll your own. The script below requires config.yml, which specifies (1) the mapping between channel names in Index.idx.xml and the channel names in the CellProfiler pipelines (2) metadata to extract from Index.idx.xml. Ensure that all the metadata fields defined in config.yml are present in the Index.idx.xml. cd ~/efs/${PROJECT_NAME}/workspace/software/cellpainting_scripts/ pyenv shell 2.7.12 parallel \\ --max-procs ${MAXPROCS} \\ --eta \\ --joblog ../../log/${BATCH_ID}/create_csv_from_xml.log \\ --results ../../log/${BATCH_ID}/create_csv_from_xml \\ --files \\ --keep-order \\ ./create_csv_from_xml.sh \\ -b ${BATCH_ID} \\ --plate {1} :::: ${PLATES} cd ../../ This is the resulting structure of load_data_csv on EFS (one level below workspace). Files for only SQ00015167 are shown. └── load_data_csv    └── 2016_04_01_a549_48hr_batch1    └── SQ00015167    ├── load_data.csv    └── load_data_with_illum.csv load_data.csv will be used by illum.cppipe. load_data_with_illum.csv will be used by analysis.cppipe. When creating load_data_with_illum.csv, the script assumes a specific location for the output folder, discussed below (see discussion on --illum_pipeline_name option). "],
["setup-jobs.html", "Chapter 4 Setup jobs 4.1 Illumination correction 4.2 Quality control 4.3 Analysis", " Chapter 4 Setup jobs For each of the pipelines, we create the following a CellProfiler batchfile a configuration file for running using Distributed-CellProfiler a docker commands file that can be run directly on the EC2 node 4.1 Illumination correction cd ~/efs/${PROJECT_NAME}/workspace/software/cellpainting_scripts/ pyenv shell 3.5.1 TMPDIR=/tmp # change this to ~/ebs_tmp if CellProfiler will be run directly on this node. parallel \\ --max-procs ${MAXPROCS} \\ --eta \\ --joblog ../../log/${BATCH_ID}/create_batch_files_illum.log \\ --results ../../log/${BATCH_ID}/create_batch_files_illum \\ --files \\ --keep-order \\ ./create_batch_files.sh \\ -b ${BATCH_ID} \\ --plate {1} \\ --datafile_filename load_data.csv \\ --output_dir ../../../${BATCH_ID}/illum \\ --pipeline ../../pipelines/${PIPELINE_SET}/illum.cppipe \\ --cp_docker_image shntnu/cellprofiler:2.2.1 \\ --create_dcp_config \\ --s3_bucket imaging-platform-dev \\ --tmpdir ${TMPDIR} :::: ${PLATES} cd ../../ This is the resulting structure of batchfiles on EFS (one level below workspace). Files for only SQ00015167 are shown. └── batchfiles/ └── 2016_04_01_a549_48hr_batch1 └── SQ00015167 └── illum ├── Batch_data.h5 ├── dcp_config.json ├── cp_docker_commands.txt └── cpgroups.csv dcp_config.json is the configuration file for running Distributed-CellProfiler. Here it is for SQ00015167: { &quot;pipeline&quot;: &quot;projects/2015_10_05_DrugRepurposing_AravindSubramanian_GolubLab_Broad/workspace/github/imaging-platform-pipelines/cellpainting_a549_20x_phenix_bin1/illum_without_batchfile.cppipe&quot;, &quot;data_file&quot;: &quot;projects/2015_10_05_DrugRepurposing_AravindSubramanian_GolubLab_Broad/workspace/load_data_csv/2016_04_01_a549_48hr_batch1/SQ00015167/load_data.csv&quot;, &quot;input&quot;: &quot;dummy&quot;, &quot;output&quot;: &quot;/home/ubuntu/bucket/projects/2015_10_05_DrugRepurposing_AravindSubramanian_GolubLab_Broad/2016_04_01_a549_48hr_batch1/illum/SQ00015167&quot;, &quot;groups&quot;: [ { &quot;Metadata&quot;: &quot;Metadata_Plate=SQ00015167&quot; } ] } cp_docker_commands.txt contains the command to compute the illumination functions on the current EC2 instance. The content of cp_docker_commands.txt for SQ00015167 is: docker run --rm --volume=/home/ubuntu/efs/2015_10_05_DrugRepurposing_AravindSubramanian_GolubLab_Broad/workspace/github/imaging-platform-pipelines/cellpainting_20x_phenix_bin1:/pipeline_dir --volume=/2016_04_01_a549_48hr_batch1/SQ00015167:/filelist_dir --volume=/home/ubuntu/efs/2015_10_05_DrugRepurposing_AravindSubramanian_GolubLab_Broad/workspace/load_data_csv/2016_04_01_a549_48hr_batch1/SQ00015167:/datafile_dir --volume=/home/ubuntu/efs/2015_10_05_DrugRepurposing_AravindSubramanian_GolubLab_Broad/workspace/batchfiles/2016_04_01_a549_48hr_batch1/SQ00015167/illum:/batchfile_dir --volume=/home/ubuntu/efs/2015_10_05_DrugRepurposing_AravindSubramanian_GolubLab_Broad/2016_04_01_a549_48hr_batch1/illum/SQ00015167:/output_dir --volume=/tmp:/tmp_dir --volume=/home/ubuntu/efs/2015_10_05_DrugRepurposing_AravindSubramanian_GolubLab_Broad/workspace/status/2016_04_01_a549_48hr_batch1/SQ00015167/illum:/status_dir --volume=/home/ubuntu/bucket/:/home/ubuntu/bucket/ --log-driver=awslogs --log-opt awslogs-group=SQ00015167 --log-opt awslogs-stream=SQ00015167 shntnu/cellprofiler -p /batchfile_dir/Batch_data.h5 -g Metadata_Plate=SQ00015167 --data-file=/datafile_dir/load_data.csv -o /output_dir -t /tmp_dir -d /status_dir/SQ00015167.txt 4.2 Quality control 4.3 Analysis cd ~/efs/${PROJECT_NAME}/workspace/software/cellpainting_scripts/ pyenv shell 3.5.1 TMPDIR=/tmp # change this to ~/ebs_tmp if CellProfiler will be run directly on this node. parallel \\ --max-procs ${MAXPROCS} \\ --eta \\ --joblog ../../log/${BATCH_ID}/create_batch_files_analysis.log \\ --results ../../log/${BATCH_ID}/create_batch_files_analysis \\ --files \\ --keep-order \\ ./create_batch_files.sh -b ${BATCH_ID} \\ --plate {1} \\ --datafile_filename load_data_with_illum.csv \\ --pipeline ../../pipelines/${PIPELINE_SET}/analysis.cppipe \\ --cp_docker_image shntnu/cellprofiler:2.2.1 \\ --create_dcp_config \\ --s3_bucket imaging-platform-dev \\ --tmpdir ${TMPDIR} :::: ${PLATES} cd ../../ This is the resulting structure of batchfiles on EFS (one level below workspace). Files for only SQ00015167 are shown. └── batchfiles/ └── 2016_04_01_a549_48hr_batch1 └── SQ00015167 └── analysis ├── Batch_data.h5 ├── dcp_config.json ├── cp_docker_commands.txt └── cpgroups.csv dcp_config.json is the configuration file for running Distributed-CellProfiler. Here it is for SQ00015167: { &quot;pipeline&quot;: &quot;projects/2015_10_05_DrugRepurposing_AravindSubramanian_GolubLab_Broad/workspace/github/imaging-platform-pipelines/cellpainting_a549_20x_phenix_bin1/analysis_without_batchfile.cppipe&quot;, &quot;data_file&quot;: &quot;projects/2015_10_05_DrugRepurposing_AravindSubramanian_GolubLab_Broad/workspace/load_data_csv/2016_04_01_a549_48hr_batch1/SQ00015167/load_data_with_illum.csv&quot;, &quot;input&quot;: &quot;dummy&quot;, &quot;output&quot;: &quot;/home/ubuntu/bucket/projects/2015_10_05_DrugRepurposing_AravindSubramanian_GolubLab_Broad/workspace/analysis/2016_04_01_a549_48hr_batch1/SQ00015167/analysis&quot;, &quot;groups&quot;: [ { &quot;Metadata&quot;: &quot;Metadata_Plate=SQ00015167,Metadata_Well=A01,Metadata_Site=1&quot; }, ... ] } Note that, as with illumination correction, this assumes that the pipeline has a _without_batchfile version of it in the same directory. cp_docker_commands.txt contains the commands to run the analysis pipeline on the current EC2 instance. In this example, the images were grouped by Plate, Well, and Site, and therefore each command processes a single site (=1 image). The first line of cp_docker_commands.txt for SQ00015167 is: docker run --rm --volume=/home/ubuntu/efs/2015_10_05_DrugRepurposing_AravindSubramanian_GolubLab_Broad/workspace/github/imaging-platform-pipelines/cellpainting_a549_20x_phenix_bin1:/pipeline_dir --volume=/2016_04_01_a549_48hr_batch1/SQ00015167:/filelist_dir --volume=/home/ubuntu/efs/2015_10_05_DrugRepurposing_AravindSubramanian_GolubLab_Broad/workspace/load_data_csv/2016_04_01_a549_48hr_batch1/SQ00015167:/datafile_dir --volume=/home/ubuntu/efs/2015_10_05_DrugRepurposing_AravindSubramanian_GolubLab_Broad/workspace/batchfiles/2016_04_01_a549_48hr_batch1/SQ00015167/analysis:/batchfile_dir --volume=/home/ubuntu/efs/2015_10_05_DrugRepurposing_AravindSubramanian_GolubLab_Broad/workspace/analysis/2016_04_01_a549_48hr_batch1/SQ00015167/analysis:/output_dir --volume=/home/ubuntu/efs/tmp/:/tmp_dir --volume=/home/ubuntu/efs/2015_10_05_DrugRepurposing_AravindSubramanian_GolubLab_Broad/workspace/status/2016_04_01_a549_48hr_batch1/SQ00015167/analysis:/status_dir --volume=/home/ubuntu/bucket/:/home/ubuntu/bucket/ --log-driver=awslogs --log-opt awslogs-group=SQ00015167 --log-opt awslogs-stream=SQ00015167-A01-1 shntnu/cellprofiler:2.2.1 -p /batchfile_dir/Batch_data.h5 -g Metadata_Plate=SQ00015167,Metadata_Well=A01,Metadata_Site=1 --data-file=/datafile_dir/load_data_with_illum.csv -o /output_dir -t /tmp_dir -d /status_dir/SQ00015167-A01-1.txt "],
["run-jobs.html", "Chapter 5 Run jobs 5.1 Illumination correction 5.2 Quality control 5.3 Analysis", " Chapter 5 Run jobs 5.1 Illumination correction 5.1.1 Single node To compute illumination functions directly on the EC2 node, run the contents of cp_docker_commands.txt for each plate for PLATE_ID in $(cat ${PLATES}); do parallel -a ../../batchfiles/${BATCH_ID}/${PLATE_ID}/illum/cp_docker_commands.txt done If this is run on the current node, this is the resulting structure of analysis, containing the output of illum.cppipe, on EFS (one level below workspace). Files for only SQ00015167 are shown. └── 2016_04_01_a549_48hr_batch1 └── illum └── SQ00015167 ├── SQ00015167_IllumAGP.mat ├── SQ00015167_IllumDNA.mat ├── SQ00015167_IllumER.mat ├── SQ00015167_IllumMito.mat ├── SQ00015167_IllumRNA.mat └── SQ00015167.stderr Sync this folder to S3, maintaining the same structure. If you used DCP to run this pipeline (discussed below), the files will have been stored directly on S3, in which case there’s no need to do a sync. cd ~/efs/${PROJECT_NAME}/ aws s3 sync ${BATCH_ID}/illum/${PLATE_ID} s3://${BUCKET}/projects/${PROJECT_NAME}/${BATCH_ID}/illum/${PLATE_ID} 5.1.2 DCP Edit the config files illum_config.py and illum_config.json in cellpainting_scripts/dcp_config_files/ as needed. Then copy to the DCP directory and setup the compute environment cd ~/efs/${PROJECT_NAME}/workspace/software/Distributed-CellProfiler/ pyenv shell 2.7.12 cp ../cellpainting_scripts/dcp_config_files/illum_config.py config.py fab setup Submit jobs and start the cluster, then monitor: parallel \\ python run.py submitJob \\ ~/efs/${PROJECT_NAME}/workspace/batchfiles/${BATCH_ID}/{1}/illum/dcp_config.json :::: ${PLATES} python run.py \\ startCluster \\ ../cellpainting_scripts/dcp_config_files/illum_config.json # do this in a tmux session. Replace `APP_NAME` the value of APP_NAME in `illum_config.py` python run.py monitor files/APP_NAMESpotFleetRequestId.json 5.2 Quality control 5.2.1 Process QC results into a database for CPA cd ~/efs/${PROJECT_NAME}/workspace/software/ git clone https://username@github.com/cytomining/cytominer-database cd cytominer-database pyenv shell 3.5.1 pip install -e . cd ~/efs/${PROJECT_NAME}/workspace mkdir qc cytominer-database ingest ~/bucket/projects/${PROJECT_NAME}/workspace/qc/${BATCH_ID}/results sqlite:///qc/${BATCH_ID}_QC.sqlite -c software/cytominer-database/cytominer_database/config/config_default.ini --no-munge rsync qc/${BATCH_ID}_QC.sqlite ~/bucket/projects/${PROJECT_NAME}/workspace/qc/${BATCH_ID}_QC.sqlite You can then download the database to your local machine; to update the S3 image paths to your local image paths you’ll need to configure and execute the following SQL statement (DB Browser for SQLite, for example, allows you to do this easily in the GUI). You need only specify the parts of the paths that are different, not the whole path. UPDATE Image SET PathName_OrigBrightfield= REPLACE(PathName_OrigBrightfield, &#39;/home/ubuntu/bucket/projects/s3/path/to/files/&#39;, &#39;/local/path/to/files/&#39;) WHERE PathName_OrigBrightfield LIKE &#39;%/home/ubuntu/bucket/projects/%&#39;; UPDATE Image SET PathName_OrigAGP= REPLACE(PathName_OrigAGP, &#39;/home/ubuntu/bucket/projects/s3/path/to/files/&#39;, &#39;/local/path/to/files/&#39;) WHERE PathName_OrigAGP LIKE &#39;%/home/ubuntu/bucket/projects/%&#39;; UPDATE Image SET PathName_OrigDNA= REPLACE(PathName_OrigDNA, &#39;/home/ubuntu/bucket/projects/s3/path/to/files/&#39;, &#39;/local/path/to/files/&#39;) WHERE PathName_OrigDNA LIKE &#39;%/home/ubuntu/bucket/projects/%&#39;; UPDATE Image SET PathName_OrigER= REPLACE(PathName_OrigER, &#39;/home/ubuntu/bucket/projects/s3/path/to/files/&#39;, &#39;/local/path/to/files/&#39;) WHERE PathName_OrigER LIKE &#39;%/home/ubuntu/bucket/projects/%&#39;; UPDATE Image SET PathName_OrigMito= REPLACE(PathName_OrigMito, &#39;/home/ubuntu/bucket/projects/s3/path/to/files/&#39;, &#39;/local/path/to/files/&#39;) WHERE PathName_OrigMito LIKE &#39;%/home/ubuntu/bucket/projects/%&#39;; UPDATE Image SET PathName_OrigRNA= REPLACE(PathName_OrigRNA, &#39;/home/ubuntu/bucket/projects/s3/path/to/files/&#39;, &#39;/local/path/to/files/&#39;) WHERE PathName_OrigRNA LIKE &#39;%/home/ubuntu/bucket/projects/%&#39; Windows users must also then execute the following statements to change the direction of any slashes in the path. UPDATE Image SET PathName_OrigBrightfield= REPLACE(PathName_OrigBrightfield, &#39;/&#39;, &#39;\\&#39;) WHERE PathName_OrigBrightfield LIKE &#39;%/%&#39;; UPDATE Image SET PathName_OrigAGP= REPLACE(PathName_OrigAGP, &#39;/&#39;, &#39;\\&#39;) WHERE PathName_OrigAGP LIKE &#39;%/%&#39;; UPDATE Image SET PathName_OrigDNA= REPLACE(PathName_OrigDNA, &#39;/&#39;, &#39;\\&#39;) WHERE PathName_OrigDNA LIKE &#39;%/%&#39;; UPDATE Image SET PathName_OrigER= REPLACE(PathName_OrigER, &#39;/&#39;, &#39;\\&#39;) WHERE PathName_OrigER LIKE &#39;%/%&#39;; UPDATE Image SET PathName_OrigMito= REPLACE(PathName_OrigMito, &#39;/&#39;, &#39;\\&#39;) WHERE PathName_OrigMito LIKE &#39;%/%&#39;; UPDATE Image SET PathName_OrigRNA= REPLACE(PathName_OrigRNA, &#39;/&#39;, &#39;\\&#39;) WHERE PathName_OrigRNA LIKE &#39;%/%&#39; You can now configure your CPA properties file with the name of your new database and perform the QC. For more information on this process, see the CellProfiler/tutorials repo. 5.3 Analysis 5.3.1 Single node To run the analysis pipeline directly on the EC2 node, run the contents of cp_docker_commands.txt for each plate for PLATE_ID in $(cat ${PLATES}); do parallel -a ../../batchfiles/${BATCH_ID}/${PLATE_ID}/analysis/cp_docker_commands.txt done If this is run on the EC2 node, this is the resulting structure of analysis, containing the output of analysis.cppipe, on EFS (one level below workspace). Files for only SQ00015167 are shown. └── analysis    └── 2016_04_01_a549_48hr_batch1    └── SQ00015167       └── analysis       └── A01-1 ├── Cells.csv ├── Cytoplasm.csv ├── Experiment.csv ├── Image.csv ├── Nuclei.csv └── outlines ├── A01_s1--cell_outlines.png └── A01_s1--nuclei_outlines.png A01-1 is site 1 of well A01. In a 384-well plate, there will be 384*9 such folders. Note that the file Experiment.csv may get created one level above, i.e., under A01-1 (see https://github.com/CellProfiler/CellProfiler/issues/1110) Sync this folder to S3, maintaining the same structure. If you used DCP to run this pipeline (discussed below), the files will have been stored directly on S3, in which case there’s no need to do a sync. cd ~/efs/${PROJECT_NAME}/workspace/ aws s3 sync analysis/${BATCH_ID}/${PLATE_ID}/analysis s3://${BUCKET}/projects/${PROJECT_NAME}/workspace/analysis/${BATCH_ID}/${PLATE_ID}/analysis/ 5.3.2 DCP Edit the config files analysis_config.py and analysis_config.json in cellpainting_scripts/dcp_config_files/ as needed. At the very least, replace the strings VAR_AWS_ACCOUNT_NUMBER,VAR_AWS_BUCKET,VAR_SUBNET_ID,VAR_GROUP_ID,VAR_KEYNAME with appropriate values. You do so using sed: cd cellpainting_scripts/dcp_config_files/ for CONFIG_FILE in analysis_config.py analysis_config.json illum_config.py illum_config.json; do sed -i ${CONFIG_FILE} &quot;s/VAR_AWS_ACCOUNT_NUMBER/NNNNNNNNNNN/g&quot; sed -i ${CONFIG_FILE} &quot;s/VAR_AWS_BUCKET/name-of-s3-bucket/g&quot; sed -i ${CONFIG_FILE} &quot;s/VAR_SUBNET_ID/subnet-NNNNNNNN/g&quot; sed -i ${CONFIG_FILE} &quot;s/VAR_GROUP_ID/sg-NNNNNNNN/g&quot; sed -i ${CONFIG_FILE} &quot;s/VAR_KEYNAME/filename-of-key-file-without-extension/g&quot; done cd .. Copy the analysis_config.py to the DCP directory and setup the compute environment. cd ~/efs/${PROJECT_NAME}/workspace/software/Distributed-CellProfiler/ pyenv shell 2.7.12 cp ../cellpainting_scripts/dcp_config_files/analysis_config.py config.py fab setup Submit jobs and start the cluster, then monitor: parallel \\ python run.py submitJob \\ ~/efs/${PROJECT_NAME}/workspace/batchfiles/${BATCH_ID}/{1}/analysis/dcp_config.json :::: ${PLATES} python run.py \\ startCluster \\ ../cellpainting_scripts/dcp_config_files/analysis_config.json # do this in a tmux session. Replace `APP_NAME` the value of APP_NAME in `analysis_config.py` python run.py monitor files/APP_NAMESpotFleetRequestId.json "],
["configure-tools-to-create-profiles.html", "Chapter 6 Configure tools to create profiles 6.1 Download software", " Chapter 6 Configure tools to create profiles 6.1 Download software "],
["create-profiles.html", "Chapter 7 Create profiles 7.1 Create database backend 7.2 Annotate 7.3 Normalize 7.4 Select variables 7.5 Aggregate replicates 7.6 Audit 7.7 Convert to other formats 7.8 Upload data", " Chapter 7 Create profiles 7.1 Create database backend Run creation of sqlite backend as well as aggregation of measurements into per-well profiles. This process can be very slow since the files are read from s3fs/EFS. We recommend first downloading the CSVs files locally and then ingesting. To do so, you need to recreate the folder structure on EBS and then run collate.R. mkdir -p ~/ebs_tmp/${PROJECT_NAME}/workspace/software cd ~/ebs_tmp/${PROJECT_NAME}/workspace/software if [ -d cytominer_scripts ]; then rm -rf cytominer_scripts; fi git clone https://github.com/broadinstitute/cytominer_scripts.git cd cytominer_scripts pyenv local 3.5.1 The command below first calls cytominer-database ingest to create the sqlite backend, and then aggregate.R to create per-well profiles. Once complete, all files are uploaded to S3 and the local cache is deleted. mkdir -p ../../log/${BATCH_ID}/ parallel \\ --max-procs ${MAXPROCS} \\ --eta \\ --joblog ../../log/${BATCH_ID}/collate.log \\ --results ../../log/${BATCH_ID}/collate \\ --files \\ --keep-order \\ ./collate.R \\ -b ${BATCH_ID} \\ --plate {1} \\ -c ingest_config.ini \\ --tmpdir ~/ebs_tmp \\ -d \\ -r s3://${BUCKET}/projects/${PROJECT_NAME}/workspace :::: ${PLATES} *Troublshooting tip:* For pipelines that use FlagImage to skip the measurements modules if the image failed QC, the failed images will have Image.csv files with fewer columns that the rest (because columns corresponding to aggregated measurements will be absent). The ingest command will show a warning related to sqlite: expected X columns but found Y - filling the rest with NULL. This is expected behavior. This is the resulting structure of backend on S3 (one level below workspace) for SQ00015167: └── backend    └── 2016_04_01_a549_48hr_batch1 └── SQ00015167 ├── SQ00015167.csv └── SQ00015167.sqlite SQ00015167.sqlite is the per cell data and SQ00015167.csv is the aggregated per-well data. Copy these files from S3 to EFS to continue with the rest of the processing aws s3 sync --exclude &quot;*.sqlite&quot; s3://${BUCKET}/projects/${PROJECT_NAME}/workspace/backend/${BATCH_ID}/ ~/efs/${PROJECT_NAME}/workspace/backend/${BATCH_ID}/ Do a quick check to view how many rows are present in each of the aggregated per-well data. parallel \\ --no-run-if-empty \\ --keep-order \\ wc -l ../../backend/${BATCH_ID}/{1}/{1}.csv :::: ${PLATES} 7.2 Annotate First, get metadata for the plates. This should be created beforehand and be made available in S3. aws s3 sync s3://${BUCKET}/projects/${PROJECT_NAME}/workspace/metadata/${BATCH_ID}/ ~/efs/${PROJECT_NAME}/workspace/metadata/${BATCH_ID}/ This is the resulting structure of the metadata folder on EFS (one level below workspace) └── metadata    └── 2016_04_01_a549_48hr_batch1    ├── barcode_platemap.csv    └── platemap    └── C-7161-01-LM6-006.txt 2016_04_01_a549_48hr_batch1 is the batch name – the plates (and all related data) are arranged under batches, as seen below. barcode_platemap.csv is structured as shown below. Assay_Plate_Barcode and Plate_Map_Name are currently the only mandatory columns (they are used to join the metadata of the plate map with each assay plate). Each unique entry in the Plate_Map_Name should have a corresponding tab-separated file .txt file under platemap (e.g. C-7161-01-LM6-006.txt) Assay_Plate_Barcode,Plate_Map_Name SQ00015167,C-7161-01-LM6-006 The tab-separated files are plate maps and are structured like this: (This is the typical format followed by Broad Chemical Biology Platform) plate_map_name well_position broad_sample mg_per_ml mmoles_per_liter solvent C-7161-01-LM6-006 A07 BRD-K18895904-001-16-1 3.12432000000000016 9.99999999999999999 DMSO C-7161-01-LM6-006 A08 BRD-K18895904-001-16-1 1.04143999999919895 3.33333333333076923 DMSO C-7161-01-LM6-006 A09 BRD-K18895904-001-16-1 0.347146666668001866 1.11111111111538462 DMSO NOTE: - plate_map_name should be identical to the name of the file (without extension). - plate_map_name and well_position are mandatory columns. - If you have two sets of plates that have the same platemap but are plated with different cell lines, then create one plate map file for each cell line, e.g. C-7161-01-LM6-006_A549.txt, rename the plate_map_name to the name of the file (without extension), add a column cell_id, and populate it with the name of the cell line (e.g. A549). This should also be reflected in the barcode_platemap.csv file. Next, append the metadata to the aggregated per-well data. You may choose to additionally append columns from another source (“EXTERNAL_METADATA” below), which you can specify using the -j flag. cd ~/efs/${PROJECT_NAME}/workspace/software/cytominer_scripts EXTERNAL_METADATA=../../metadata/${BATCH_ID}/cell_painting_dataset_cmap_annotations_moa.csv parallel \\ --no-run-if-empty \\ --eta \\ --joblog ../../log/${BATCH_ID}/annotate.log \\ --results ../../log/${BATCH_ID}/annotate \\ --files \\ --keep-order \\ ./annotate.R \\ -b ${BATCH_ID} \\ -p {1} \\ -d \\ -j ${EXTERNAL_METADATA} \\ -m chemical :::: ${PLATES} ``` This is the resulting structure of `backend` on EFS (one level below `workspace`) for `SQ00015167`: └── backend    └── 2016_04_01_a549_48hr_batch1 └── SQ00015167 ├── SQ00015167_augmented.csv └── SQ00015167.csv ``` SQ00015167_augmented.csv is the aggregated per-well data, annotated with metadata. Do a quick check to view how many rows are present in each of the annotated per-well data. parallel \\ --no-run-if-empty \\ --keep-order \\ wc -l ../../backend/${BATCH_ID}/{1}/{1}_augmented.csv :::: ${PLATES} 7.3 Normalize Use all wells on the plate to normalize each feature. By default, this performs robust z-scoring per feature. The default input is the annotated per-well data. parallel \\ --no-run-if-empty \\ --eta \\ --joblog ../../log/${BATCH_ID}/normalize.log \\ --results ../../log/${BATCH_ID}/normalize \\ --files \\ --keep-order \\ ./normalize.R \\ -b ${BATCH_ID} \\ -p {1} \\ -s \\&quot;Metadata_broad_sample_type != \\&#39;\\&#39;\\&#39;dummy\\&#39;\\&#39;\\&#39;\\&quot; :::: ${PLATES} NOTE: - don’t escape quotes if not using parallel i.e. use -s &quot;Metadata_broad_sample_type != '''dummy'''&quot; if not using within parallel. - to use a different reference distribution to compute the median and m.a.d. for z-scoring, change the filter specified using the -s flag. This is the resulting structure of backend on EFS (one level below workspace) for SQ00015167: └── backend    └── 2016_04_01_a549_48hr_batch1 └── SQ00015167 ├── SQ00015167_augmented.csv ├── SQ00015167.csv └── SQ00015167_normalized.csv SQ00015167_normalized.csv is the robust z-scored (normalized) per-well data. Do a quick check to view how many rows are present in each of the normalized per-well data. parallel \\ --no-run-if-empty \\ --keep-order \\ wc -l ../../backend/${BATCH_ID}/{1}/{1}_normalized.csv :::: ${PLATES} 7.4 Select variables Create samples to do variable selection. Sample some wells from each replicate. Below, this is done by sample 2 entire replicate plates per platemap. Use -n to specify number of replicate plates to be used to create the sample. Samples are created for both, normalized and unnormalized data, because the variable selection techniques may require both. mkdir -p ../../parameters/${BATCH_ID}/sample/ # sample normalized data ./sample.R \\ -b ${BATCH_ID} \\ -f &quot;_normalized.csv$&quot; \\ -n 2 \\ -o ../../parameters/${BATCH_ID}/sample/${BATCH_ID}_normalized_sample.feather # sample unnormalized data ./sample.R \\ -b ${BATCH_ID} \\ -f &quot;_augmented.csv$&quot; \\ -n 2 \\ -o ../../parameters/${BATCH_ID}/sample/${BATCH_ID}_augmented_sample.feather Make a list of variables to be preserved after replicate_correlation variable selection is performed. ./preselect.R \\ -b ${BATCH_ID} \\ -i ../../parameters/${BATCH_ID}/sample/${BATCH_ID}_normalized_sample.feather \\ -r replicate_correlation \\ -s &quot;Metadata_broad_sample_type == &#39;&#39;&#39;trt&#39;&#39;&#39;&quot; \\ -n 2 Make a list of variables to be preserved after correlation_threshold variable selection is performed. ./preselect.R \\ -b ${BATCH_ID} \\ -i ../../parameters/${BATCH_ID}/sample/${BATCH_ID}_normalized_sample.feather \\ -r correlation_threshold Make a list of variables to be preserved after variance_threshold variable selection is performed. ./preselect.R \\ -b ${BATCH_ID} \\ -i ../../parameters/${BATCH_ID}/sample/${BATCH_ID}_augmented_sample.feather \\ -r variance_threshold \\ -s &quot;Metadata_broad_sample_type == &#39;&#39;&#39;control&#39;&#39;&#39;&quot; Some variables have previously identified as being noisy or non-informative. Create a list of variables that excludes these variables. # manually remove some features echo &quot;variable&quot; &gt; ../../parameters/${BATCH_ID}/variable_selection/manual.txt head -1 \\ ../../backend/${BATCH_ID}/${SAMPLE_PLATE_ID}/${SAMPLE_PLATE_ID}.csv \\ |tr &quot;,&quot; &quot;\\n&quot;|grep -v Meta|grep -E -v &#39;Granularity_14|Granularity_15|Granularity_16|Manders|RWC&#39; &gt;&gt; \\ ../../parameters/${BATCH_ID}/variable_selection/manual.txt ALTERNATIVE: You may have already performed these steps for a different batch of data, and want to simply copy the parameters to this batch: mkdir -p ../../parameters/${BATCH_ID}/variable_selection/ REFERENCE_BATCH_ID=2018_02_23_LKCP_DBG aws s3 sync \\ s3://${BUCKET}/projects/${PROJECT_NAME}/workspace/parameters/${REFERENCE_BATCH_ID}/ \\ ~/efs/${PROJECT_NAME}/workspace/parameters/${REFERENCE_BATCH_ID}/ rsync -arzv ../../parameters/${REFERENCE_BATCH_ID}/variable_selection/ ../../parameters/${BATCH_ID}/variable_selection/ The previous steps only create a list of variable to be preserved for each variable selection method. To actually apply variable selection, we compute the intersection of all these variable lists, then preserve only those columns of the normalized per-well data. parallel \\ --no-run-if-empty \\ --eta \\ --joblog ../../log/${BATCH_ID}/select.log \\ --results ../../log/${BATCH_ID}/select \\ --files \\ --keep-order \\ ./select.R \\ -b ${BATCH_ID} \\ -p {1} \\ -r variance_threshold,replicate_correlation,correlation_threshold,manual :::: ${PLATES} This is the resulting structure of backend on EFS (one level below workspace) for SQ00015167: └── backend    └── 2016_04_01_a549_48hr_batch1 └── SQ00015167 ├── SQ00015167_augmented.csv ├── SQ00015167.csv ├── SQ00015167_normalized.csv └── SQ00015167_normalized_variable_selected.csv SQ00015167_normalized_variable_selected.csv is the variable-selected version of the normalized per-well data. Do a quick check to view how many rows are present in each of the normalized per-well data. parallel \\ --no-run-if-empty \\ --keep-order \\ wc -l ../../backend/${BATCH_ID}/{1}/{1}_normalized_variable_selected.csv :::: ${PLATES} 7.5 Aggregate replicates Combine replicate plates of each plate map by averaging (mean). mkdir -p ../../collated/${BATCH_ID}/ PLATE_MAPS=../../scratch/${BATCH_ID}/plate_maps.txt csvcut -c Plate_Map_Name \\ ../../metadata/${BATCH_ID}/barcode_platemap.csv | \\ tail -n +2|sort|uniq &gt; \\ ${PLATE_MAPS} parallel \\ --no-run-if-empty \\ --eta \\ --joblog ../../log/${BATCH_ID}/collapse.log \\ --results ../../log/${BATCH_ID}/collapse \\ --keep-order \\ ./collapse.R \\ -b ${BATCH_ID} \\ -m {1} \\ -f _normalized_variable_selected.csv \\ -o ../../collated/${BATCH_ID}/{1}_collapsed.csv :::: ${PLATE_MAPS} This is the resulting structure of collated on EFS (one level below workspace) for 2016_04_01_a549_48hr_batch1: └── collated    └── 2016_04_01_a549_48hr_batch1 └── C-7161-01-LM6-006_collapsed.csv C-7161-01-LM6-006_collapsed.csv is the replicate averaged data for plate map C-7161-01-LM6-006. Do a quick check to view how many rows are present in the replicate averaged data of each plate map. parallel \\ --no-run-if-empty \\ --keep-order \\ wc -l ../../collated/${BATCH_ID}/{1}_collapsed.csv :::: ${PLATE_MAPS} Combine all averaged profiles in the batch into a single file. csvstack \\ ../../collated/${BATCH_ID}/*_collapsed.csv &gt; \\ ../../collated/${BATCH_ID}/${BATCH_ID}_collapsed.csv 7.6 Audit Audit each plate map for replicate reproducibility mkdir -p ../../audit/${BATCH_ID}/ Audit only treated wells parallel \\ --no-run-if-empty \\ --eta \\ --joblog ../../log/${BATCH_ID}/audit.log \\ --results ../../log/${BATCH_ID}/audit \\ --files \\ --keep-order \\ ./audit.R \\ -b ${BATCH_ID} \\ -m {1} \\ -f _normalized_variable_selected.csv \\ -s \\&quot;Metadata_broad_sample_type == \\&#39;\\&#39;\\&#39;trt\\&#39;\\&#39;\\&#39;\\&quot; \\ -o ../../audit/${BATCH_ID}/{1}_audit.csv \\ -l ../../audit/${BATCH_ID}/{1}_audit_detailed.csv \\ -p Metadata_Plate_Map_Name,Metadata_moa,Metadata_pert_id,Metadata_broad_sample,Metadata_mmoles_per_liter,Metadata_Well :::: ${PLATE_MAPS} Audit only control wells, i.e., how well do control wells in the same position correlate? parallel \\ --no-run-if-empty \\ --eta \\ --joblog ../../log/${BATCH_ID}/audit_control.log \\ --results ../../log/${BATCH_ID}/audit_control \\ --files \\ --keep-order \\ ./audit.R \\ -b ${BATCH_ID} \\ -m {1} \\ -f _normalized_variable_selected.csv \\ -s \\&quot;Metadata_broad_sample_type == \\&#39;\\&#39;\\&#39;control\\&#39;\\&#39;\\&#39;\\&quot; \\ -o ../../audit/${BATCH_ID}/{1}_audit_control.csv \\ -l ../../audit/${BATCH_ID}/{1}_audit_control_detailed.csv \\ -p Metadata_Well :::: ${PLATE_MAPS} 7.7 Convert to other formats Convert per-plate CSV files to GCT parallel \\ --no-run-if-empty \\ --eta \\ --joblog ../../log/${BATCH_ID}/csv2gct_backend.log \\ --results ../../log/${BATCH_ID}/csv2gct_backend \\ --files \\ --keep-order \\ ./csv2gct.R \\ ../../backend/${BATCH_ID}/{1}/{1}_{2}.csv \\ -o ../../backend/${BATCH_ID}/{1}/{1}_{2}.gct :::: ${PLATES} ::: augmented normalized normalized_variable_selected Convert per-plate map CSV files to GCT parallel \\ --no-run-if-empty \\ --eta \\ --joblog ../../log/${BATCH_ID}/csv2gct_collapsed.log \\ --results ../../log/${BATCH_ID}/csv2gct_collapsed \\ --files \\ --keep-order \\ ./csv2gct.R \\ ../../collated/${BATCH_ID}/{1}_collapsed.csv \\ -o ../../collated/${BATCH_ID}/{1}_collapsed.gct :::: ${PLATE_MAPS} Convert collapsed to gct ./csv2gct.R \\ ../../collated/${BATCH_ID}/${BATCH_ID}_collapsed.csv \\ -o ../../collated/${BATCH_ID}/${BATCH_ID}_collapsed.gct 7.8 Upload data 7.8.1 Sync to S3 parallel \\ aws s3 sync \\ ../../{1}/${BATCH_ID}/ \\ s3://imaging-platform-dev/projects/${PROJECT_NAME}/workspace/{1}/${BATCH_ID}/ ::: audit backend batchfiles collated load_data_csv log metadata parameters scratch 7.8.2 Sync down from S3 onto a machine Specify location for syncing BROAD_NFS=/cmap/imaging Set variables PROJECT_NAME=2015_10_05_DrugRepurposing_AravindSubramanian_GolubLab_Broad BATCH_ID=2016_04_01_a549_48hr_batch1 Sync the files echo audit backend batchfiles collated load_data_csv log metadata parameters scratch | \\ tr &quot; &quot; &quot;\\n&quot; | xargs -I % \\ aws s3 sync \\ --exclude &quot;*.sqlite&quot; \\ s3://imaging-platform-dev/projects/${PROJECT_NAME}/workspace/%/${BATCH_ID}/ \\ ${BROAD_NFS}/${PROJECT_NAME}/workspace/%/${BATCH_ID}/ "],
["appendix-a.html", "A Appendix A A.1 Directory structure", " A Appendix A A.1 Directory structure ├── 2016_04_01_a549_48hr_batch1 │ ├── illum │ │ └── SQ00015167 │ │ ├── SQ00015167_IllumAGP.mat │ │ ├── SQ00015167_IllumDNA.mat │ │ ├── SQ00015167_IllumER.mat │ │ ├── SQ00015167_IllumMito.mat │ │ ├── SQ00015167_IllumRNA.mat │ │ └── SQ00015167.stderr │ └── images │ └── SQ00015167__2016-04-21T03_34_00-Measurement1 │ ├── Assaylayout │ ├── FFC_Profile │ └── Images │ ├── r01c01f01p01-ch1sk1fk1fl1.tiff │ ├── r01c01f01p01-ch2sk1fk1fl1.tiff │ ├── r01c01f01p01-ch3sk1fk1fl1.tiff │ ├── r01c01f01p01-ch4sk1fk1fl1.tiff │ └── r01c01f01p01-ch5sk1fk1fl1.tiff └── workspace ├── audit │    └── 2016_04_01_a549_48hr_batch1 ├── analysis │    └── 2016_04_01_a549_48hr_batch1 │    └── SQ00015167 │       └── analysis │       └── A01-1 │ ├── Cells.csv │ ├── Cytoplasm.csv │ ├── Experiment.csv │ ├── Image.csv │ ├── Nuclei.csv │ └── outlines │ └── SQ00015167 │ ├── A01_s1--cell_outlines.png │ └── A01_s1--nuclei_outlines.png ├── backend │   └── 2016_04_01_a549_48hr_batch1 │   └── SQ00015167 │ ├── SQ00015167.csv │ └── SQ00015167.sqlite ├── batchfiles │ └── 2016_04_01_a549_48hr_batch1 │ └── SQ00015167 │ ├── analysis │ │ ├── Batch_data.h5 │ │ ├── dcp_config.json │ │ ├── cp_docker_commands.txt │ │ └── cpgroups.csv │ └── illum │ ├── Batch_data.h5 │ ├── dcp_config.json │ ├── cp_docker_commands.txt │ └── cpgroups.csv ├── github │   └── imaging-platform-pipelines ├── images │   └── 2016_04_01_a549_48hr_batch1 -&gt; /home/ubuntu/bucket/projects/2015_10_05_DrugRepurposing_AravindSubramanian_GolubLab_Broad/2016_04_01_a549_48hr_batch1/images/ ├── load_data_csv │   └── 2016_04_01_a549_48hr_batch1 │   └── SQ00015167 │   ├── load_data.csv │   └── load_data_with_illum.csv ├── log │   ├── create_batch_files_analysis │   ├── create_batch_files_illum │   ├── create_csv_from_xml │   └── collate   ├── metadata │   └── 2016_04_01_a549_48hr_batch1 │   ├── barcode_platemap.csv │   └── platemap │   └── C-7161-01-LM6-006.txt ├── pipelines -&gt; github/imaging-platform-pipelines ├── status └── software ├── cellpainting_scripts ├── cytominer_scripts └── pe2loaddata "]
]
